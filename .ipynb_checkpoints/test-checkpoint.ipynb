{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests file\n",
    "\n",
    "In this file we will make performance and consistency tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!conda install --yes --prefix {sys.prefix} -c conda-forge gensim\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import Globals.globals as glob\n",
    "import SearchAlgorithms.searchAlgorithms as algo\n",
    "from Tokenization import tokenizer\n",
    "from Tokenization.tokenizer import createListOfTokens, replaceWordsByStem, replaceWordsByLemma, removeStopWords\n",
    "from QueryMaker.queryShell import processQueryString\n",
    "from DocumentServer import documentServer\n",
    "from Tokenization.TokenizationCpp import tokenizer as tokenizerCpp\n",
    "from IFConstruction import ifConstructor\n",
    "\n",
    "datasetFoldername = \"../latimes\"\n",
    "documentServer.foldername = datasetFoldername\n",
    "glob.loadDocID2Content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Impact of the word score on the top 10 documents\n",
    "\n",
    "Setting : \n",
    "    - search algorithm : naive,\n",
    "    - inverted file : no stemming and no lemmatization,\n",
    "    - query processing : no stemming, no lemmatization, no word embedding.\n",
    "    - query : \"Chocolate and internet\"\n",
    "    \n",
    "Variable parameter : **word score âˆˆ {<number of occurence\\>, <tf * idf>}**\n",
    " \n",
    "In this section, we compute the top 10 results with the naive algorithm using an inverted file which has been built without any stemming, lemmatization and no word embedding is applied on the query.\n",
    "We think \"Chocolate and internet\" is a relevant query to test the word score since there is a significant difference between the number of occurence of \"chocolate\" and \"internet\" in the dataset as shown further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chocolate', 3), ('internet', 3)]\n"
     ]
    }
   ],
   "source": [
    "searchAlgorithm = algo.naiveAlgo\n",
    "query = \"Chocolate and internet\"\n",
    "query = processQueryString(query, stemming = False, lemmatization = False, embedding = False)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : because no word embedding is used, the reader must ignore the weights paired with the words. The weights are not used by the naive algorithm anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **First test : word score = number of occurence** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary set has a size of 289292\n",
      "len(choco_PL) : 723\n",
      "len(internet_PL) : 4\n",
      "list(choco_PL.items())[:4] : [('321713', 38), ('145821', 27), ('321712', 25), ('111', 24)]\n",
      "list(internet_PL.items())[:4] : [('85032', 8), ('85141', 6), ('105932', 1), ('254071', 1)]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_filename = \"Globals/nostemm_nolemm_notfidf/vocabulary.dict\"\n",
    "IF_filename = \"Globals/nostemm_nolemm_notfidf/IF.dict\"\n",
    "\n",
    "glob.loadVocabulary(vocabulary_filename, IF_filename)\n",
    "\n",
    "choco_PL = glob.voc2PostingList(\"chocolate\")\n",
    "internet_PL = glob.voc2PostingList(\"internet\")\n",
    "\n",
    "print(\"len(choco_PL) :\", len(choco_PL))\n",
    "print(\"len(internet_PL) :\", len(internet_PL))\n",
    "\n",
    "print(\"list(choco_PL.items())[:4] :\", list(choco_PL.items())[:4])\n",
    "print(\"list(internet_PL.items())[:4] :\", list(internet_PL.items())[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : we can observe that \"chocolate\" appears in more documents and with a bigger number of occurence in each documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----------------------------------\n",
      "DOCID : 321713\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : GOOD COOKING: MAKE YOUR HOLIDAY INDULGENCE BITTERSWEET \n",
      "\n",
      "2 ----------------------------------\n",
      "DOCID : 145821\n",
      "\n",
      "DATE : December 8, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Orange County Life; Part N; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : SHE FINDS SWEET SUCCESS WITH CHOCOLATES \n",
      "\n",
      "3 ----------------------------------\n",
      "DOCID : 321712\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : BACK TO BASICS: DON'T BE AFRAID: IT'S SIMPLY PERFECT CHOCOLATE \n",
      "\n",
      "4 ----------------------------------\n",
      "DOCID : 111\n",
      "\n",
      "DATE : January 1, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Opinion; Part 5; Page 5; Column 1; Op-Ed Desk \n",
      "\n",
      "HEADLINE : LITTLE CHOCOLATE DOUGHNUTS TELL THE TALE OF THE U.S. TRADE CRISIS \n",
      "\n",
      "5 ----------------------------------\n",
      "DOCID : 196334\n",
      "\n",
      "DATE : March 29, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 12; Column 2 \n",
      "\n",
      "HEADLINE : CHOCOLATE TILES SWEETEN DESSERT TABLE \n",
      "\n",
      "6 ----------------------------------\n",
      "DOCID : 236382\n",
      "\n",
      "DATE : June 21, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : CULINARY SOS: FISHING FOR FROMIN'S SALAD RECIPE \n",
      "\n",
      "7 ----------------------------------\n",
      "DOCID : 265773\n",
      "\n",
      "DATE : August 23, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 18; Column 2 \n",
      "\n",
      "HEADLINE : THE BIRTH OF A PIE RECIPE: IT STARTED WITH A KISS \n",
      "\n",
      "8 ----------------------------------\n",
      "DOCID : 179442\n",
      "\n",
      "DATE : February 22, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 2; Column 1 \n",
      "\n",
      "HEADLINE : SWEETS, CHOCOLATE AND CALIFORNIANS DOMINATE 34TH PILLSBURY BAKE-OFF; \n",
      "</P>\n",
      "<P>\n",
      "CONTEST: PETALUMA TAX ACCOUNTANT TAKES TOP PRIZE IN PILLSBURY BAKE-OFF. \n",
      "\n",
      "9 ----------------------------------\n",
      "DOCID : 324682\n",
      "\n",
      "DATE : December 20, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 40; Column 1 \n",
      "\n",
      "HEADLINE : SHOPPING: 'TIS THE SEASON TO EAT CHOCOLATE \n",
      "\n",
      "10 ----------------------------------\n",
      "DOCID : 233452\n",
      "\n",
      "DATE : June 14, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : SWEET DREAMS \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = searchAlgorithm(query)\n",
    "\n",
    "content_result = documentServer.serveDocuments(result)\n",
    "\n",
    "for idx, doc in enumerate(content_result.keys()):\n",
    "\tprint(idx+1,\"----------------------------------\")\n",
    "\tprint(content_result[doc][\"metadata\"]),\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : if we look at the headlines of the top 10 results we can clearly see that they all are related to \"chocolate\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Second test : word score = tf * idf = (1 + log(number of occurrences)) * log(total number of documents/(1 + length of posting list)))** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary set has a size of 290045\n",
      "len(choco_PL) : 724\n",
      "len(internet_PL) : 4\n",
      "list(choco_PL.items())[:4] : [('321713', 24.139), ('145821', 22.36), ('321712', 21.959), ('111', 21.747)]\n",
      "list(internet_PL.items())[:4] : [('85032', 32.037), ('85141', 29.044), ('105932', 10.403), ('254071', 10.403)]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_filename = \"Globals/nostemm_nolemm_tf_idf/vocabulary.dict\"\n",
    "IF_filename = \"Globals/nostemm_nolemm_tf_idf/IF.dict\"\n",
    "\n",
    "glob.loadVocabulary(vocabulary_filename, IF_filename)\n",
    "\n",
    "choco_PL = glob.voc2PostingList(\"chocolate\")\n",
    "internet_PL = glob.voc2PostingList(\"internet\")\n",
    "\n",
    "print(\"len(choco_PL) :\", len(choco_PL))\n",
    "print(\"len(internet_PL) :\", len(internet_PL))\n",
    "\n",
    "print(\"list(choco_PL.items())[:4] :\", list(choco_PL.items())[:4])\n",
    "print(\"list(internet_PL.items())[:4] :\", list(internet_PL.items())[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : we can observe that, even if \"chocolate\" appears in more documents and with a bigger number of occurence in each documents (as seen before), the new score computation makes \"internet\" reach higher scores than \"chocolate\" in some documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----------------------------------\n",
      "DOCID : 85032\n",
      "\n",
      "DATE : July 21, 1989, Friday, Home Edition \n",
      "\n",
      "SECTION : Part 1; Page 14; Column 5; National Desk \n",
      "\n",
      "HEADLINE : COMPUTER NETWORK SEEN AS STILL VULNERABLE TO VIRUSES \n",
      "\n",
      "2 ----------------------------------\n",
      "DOCID : 85141\n",
      "\n",
      "DATE : July 21, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Business; Part 4; Page 3; Column 5; Financial Desk \n",
      "\n",
      "HEADLINE : PLAN SOUGHT TO KEEP 'VIRUSES' FROM A COMPUTER NETWORK \n",
      "\n",
      "3 ----------------------------------\n",
      "DOCID : 321713\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : GOOD COOKING: MAKE YOUR HOLIDAY INDULGENCE BITTERSWEET \n",
      "\n",
      "4 ----------------------------------\n",
      "DOCID : 145821\n",
      "\n",
      "DATE : December 8, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Orange County Life; Part N; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : SHE FINDS SWEET SUCCESS WITH CHOCOLATES \n",
      "\n",
      "5 ----------------------------------\n",
      "DOCID : 321712\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : BACK TO BASICS: DON'T BE AFRAID: IT'S SIMPLY PERFECT CHOCOLATE \n",
      "\n",
      "6 ----------------------------------\n",
      "DOCID : 111\n",
      "\n",
      "DATE : January 1, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Opinion; Part 5; Page 5; Column 1; Op-Ed Desk \n",
      "\n",
      "HEADLINE : LITTLE CHOCOLATE DOUGHNUTS TELL THE TALE OF THE U.S. TRADE CRISIS \n",
      "\n",
      "7 ----------------------------------\n",
      "DOCID : 196334\n",
      "\n",
      "DATE : March 29, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 12; Column 2 \n",
      "\n",
      "HEADLINE : CHOCOLATE TILES SWEETEN DESSERT TABLE \n",
      "\n",
      "8 ----------------------------------\n",
      "DOCID : 236382\n",
      "\n",
      "DATE : June 21, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : CULINARY SOS: FISHING FOR FROMIN'S SALAD RECIPE \n",
      "\n",
      "9 ----------------------------------\n",
      "DOCID : 265773\n",
      "\n",
      "DATE : August 23, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 18; Column 2 \n",
      "\n",
      "HEADLINE : THE BIRTH OF A PIE RECIPE: IT STARTED WITH A KISS \n",
      "\n",
      "10 ----------------------------------\n",
      "DOCID : 179442\n",
      "\n",
      "DATE : February 22, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 2; Column 1 \n",
      "\n",
      "HEADLINE : SWEETS, CHOCOLATE AND CALIFORNIANS DOMINATE 34TH PILLSBURY BAKE-OFF; \n",
      "</P>\n",
      "<P>\n",
      "CONTEST: PETALUMA TAX ACCOUNTANT TAKES TOP PRIZE IN PILLSBURY BAKE-OFF. \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = searchAlgorithm(query)\n",
    "\n",
    "content_result = documentServer.serveDocuments(result)\n",
    "\n",
    "for idx, doc in enumerate(content_result.keys()):\n",
    "\tprint(idx+1,\"----------------------------------\")\n",
    "\tprint(content_result[doc][\"metadata\"]),\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : now, if we look at the headlines of the top 10 results, both \"chocolate\" and \"internet\" seem to be represented in the results. However, it is not obvious why documents related to \"internet\" should be better than the ones related to \"chocolate\". This behavior is due to the naive algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The score tf * idf shows itself more relevant than a simple word occurence counter since it allows rarer words to be considered by the algorithm and it lower the importance of common words. From now on, our tests will only use this score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Impact of the search algorithm on the top 10 documents\n",
    "\n",
    "Setting : \n",
    "    - word score : tf * idf,\n",
    "    - inverted file : no stemming and no lemmatization,\n",
    "    - query processing : no stemming, no lemmatization, no word embedding.\n",
    "    - query : \"Chocolate and internet\"\n",
    "\n",
    "Variable parameter : **search algorithm âˆˆ {<naive\\>, <fagin\\>, <treshold\\>}**\n",
    "\n",
    "In this section, we compute the top 10 results with two new search algorithms using an inverted file which has been built without any stemming, lemmatization and no word embedding is applied on the query.\n",
    "We still use \"Chocolate and internet\" as the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from the naive algorithm have already been computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----------------------------------\n",
      "DOCID : 85032\n",
      "\n",
      "DATE : July 21, 1989, Friday, Home Edition \n",
      "\n",
      "SECTION : Part 1; Page 14; Column 5; National Desk \n",
      "\n",
      "HEADLINE : COMPUTER NETWORK SEEN AS STILL VULNERABLE TO VIRUSES \n",
      "\n",
      "2 ----------------------------------\n",
      "DOCID : 85141\n",
      "\n",
      "DATE : July 21, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Business; Part 4; Page 3; Column 5; Financial Desk \n",
      "\n",
      "HEADLINE : PLAN SOUGHT TO KEEP 'VIRUSES' FROM A COMPUTER NETWORK \n",
      "\n",
      "3 ----------------------------------\n",
      "DOCID : 321713\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : GOOD COOKING: MAKE YOUR HOLIDAY INDULGENCE BITTERSWEET \n",
      "\n",
      "4 ----------------------------------\n",
      "DOCID : 145821\n",
      "\n",
      "DATE : December 8, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Orange County Life; Part N; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : SHE FINDS SWEET SUCCESS WITH CHOCOLATES \n",
      "\n",
      "5 ----------------------------------\n",
      "DOCID : 321712\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : BACK TO BASICS: DON'T BE AFRAID: IT'S SIMPLY PERFECT CHOCOLATE \n",
      "\n",
      "6 ----------------------------------\n",
      "DOCID : 111\n",
      "\n",
      "DATE : January 1, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Opinion; Part 5; Page 5; Column 1; Op-Ed Desk \n",
      "\n",
      "HEADLINE : LITTLE CHOCOLATE DOUGHNUTS TELL THE TALE OF THE U.S. TRADE CRISIS \n",
      "\n",
      "7 ----------------------------------\n",
      "DOCID : 196334\n",
      "\n",
      "DATE : March 29, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 12; Column 2 \n",
      "\n",
      "HEADLINE : CHOCOLATE TILES SWEETEN DESSERT TABLE \n",
      "\n",
      "8 ----------------------------------\n",
      "DOCID : 236382\n",
      "\n",
      "DATE : June 21, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : CULINARY SOS: FISHING FOR FROMIN'S SALAD RECIPE \n",
      "\n",
      "9 ----------------------------------\n",
      "DOCID : 265773\n",
      "\n",
      "DATE : August 23, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 18; Column 2 \n",
      "\n",
      "HEADLINE : THE BIRTH OF A PIE RECIPE: IT STARTED WITH A KISS \n",
      "\n",
      "10 ----------------------------------\n",
      "DOCID : 179442\n",
      "\n",
      "DATE : February 22, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 2; Column 1 \n",
      "\n",
      "HEADLINE : SWEETS, CHOCOLATE AND CALIFORNIANS DOMINATE 34TH PILLSBURY BAKE-OFF; \n",
      "</P>\n",
      "<P>\n",
      "CONTEST: PETALUMA TAX ACCOUNTANT TAKES TOP PRIZE IN PILLSBURY BAKE-OFF. \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "for idx, doc in enumerate(content_result.keys()):\n",
    "\tprint(idx+1,\"----------------------------------\")\n",
    "\tprint(content_result[doc][\"metadata\"]),\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : the top 10 (doc ID) with the naive algorithm is 85032, 85141, 321713, 145821, 321712, 111, 196334, 236382, 265773, 179442."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the same query with the fagin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chocolate', 3), ('internet', 3)]\n",
      "1 ----------------------------------\n",
      "DOCID : 85032\n",
      "\n",
      "DATE : July 21, 1989, Friday, Home Edition \n",
      "\n",
      "SECTION : Part 1; Page 14; Column 5; National Desk \n",
      "\n",
      "HEADLINE : COMPUTER NETWORK SEEN AS STILL VULNERABLE TO VIRUSES \n",
      "\n",
      "2 ----------------------------------\n",
      "DOCID : 85141\n",
      "\n",
      "DATE : July 21, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Business; Part 4; Page 3; Column 5; Financial Desk \n",
      "\n",
      "HEADLINE : PLAN SOUGHT TO KEEP 'VIRUSES' FROM A COMPUTER NETWORK \n",
      "\n",
      "3 ----------------------------------\n",
      "DOCID : 321713\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : GOOD COOKING: MAKE YOUR HOLIDAY INDULGENCE BITTERSWEET \n",
      "\n",
      "4 ----------------------------------\n",
      "DOCID : 145821\n",
      "\n",
      "DATE : December 8, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Orange County Life; Part N; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : SHE FINDS SWEET SUCCESS WITH CHOCOLATES \n",
      "\n",
      "5 ----------------------------------\n",
      "DOCID : 321712\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : BACK TO BASICS: DON'T BE AFRAID: IT'S SIMPLY PERFECT CHOCOLATE \n",
      "\n",
      "6 ----------------------------------\n",
      "DOCID : 111\n",
      "\n",
      "DATE : January 1, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Opinion; Part 5; Page 5; Column 1; Op-Ed Desk \n",
      "\n",
      "HEADLINE : LITTLE CHOCOLATE DOUGHNUTS TELL THE TALE OF THE U.S. TRADE CRISIS \n",
      "\n",
      "7 ----------------------------------\n",
      "DOCID : 196334\n",
      "\n",
      "DATE : March 29, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 12; Column 2 \n",
      "\n",
      "HEADLINE : CHOCOLATE TILES SWEETEN DESSERT TABLE \n",
      "\n",
      "8 ----------------------------------\n",
      "DOCID : 236382\n",
      "\n",
      "DATE : June 21, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : CULINARY SOS: FISHING FOR FROMIN'S SALAD RECIPE \n",
      "\n",
      "9 ----------------------------------\n",
      "DOCID : 265773\n",
      "\n",
      "DATE : August 23, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 18; Column 2 \n",
      "\n",
      "HEADLINE : THE BIRTH OF A PIE RECIPE: IT STARTED WITH A KISS \n",
      "\n",
      "10 ----------------------------------\n",
      "DOCID : 179442\n",
      "\n",
      "DATE : February 22, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 2; Column 1 \n",
      "\n",
      "HEADLINE : SWEETS, CHOCOLATE AND CALIFORNIANS DOMINATE 34TH PILLSBURY BAKE-OFF; \n",
      "</P>\n",
      "<P>\n",
      "CONTEST: PETALUMA TAX ACCOUNTANT TAKES TOP PRIZE IN PILLSBURY BAKE-OFF. \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "searchAlgorithm = algo.faginAlgo\n",
    "\n",
    "print(query)\n",
    "\n",
    "result = searchAlgorithm(query)\n",
    "\n",
    "content_result = documentServer.serveDocuments(result)\n",
    "\n",
    "for idx, doc in enumerate(content_result.keys()):\n",
    "\tprint(idx+1,\"----------------------------------\")\n",
    "\tprint(content_result[doc][\"metadata\"]),\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : the top 10 (doc ID) with the fagin algorithm is 85032, 85141, 321713, 145821, 321712, 111, 196334, 236382, 265773, 179442. It is exactly the same than with the naive algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the same query with the threshold algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----------------------------------\n",
      "DOCID : 85032\n",
      "\n",
      "DATE : July 21, 1989, Friday, Home Edition \n",
      "\n",
      "SECTION : Part 1; Page 14; Column 5; National Desk \n",
      "\n",
      "HEADLINE : COMPUTER NETWORK SEEN AS STILL VULNERABLE TO VIRUSES \n",
      "\n",
      "2 ----------------------------------\n",
      "DOCID : 85141\n",
      "\n",
      "DATE : July 21, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Business; Part 4; Page 3; Column 5; Financial Desk \n",
      "\n",
      "HEADLINE : PLAN SOUGHT TO KEEP 'VIRUSES' FROM A COMPUTER NETWORK \n",
      "\n",
      "3 ----------------------------------\n",
      "DOCID : 321713\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : GOOD COOKING: MAKE YOUR HOLIDAY INDULGENCE BITTERSWEET \n",
      "\n",
      "4 ----------------------------------\n",
      "DOCID : 145821\n",
      "\n",
      "DATE : December 8, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Orange County Life; Part N; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : SHE FINDS SWEET SUCCESS WITH CHOCOLATES \n",
      "\n",
      "5 ----------------------------------\n",
      "DOCID : 321712\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : BACK TO BASICS: DON'T BE AFRAID: IT'S SIMPLY PERFECT CHOCOLATE \n",
      "\n",
      "6 ----------------------------------\n",
      "DOCID : 111\n",
      "\n",
      "DATE : January 1, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Opinion; Part 5; Page 5; Column 1; Op-Ed Desk \n",
      "\n",
      "HEADLINE : LITTLE CHOCOLATE DOUGHNUTS TELL THE TALE OF THE U.S. TRADE CRISIS \n",
      "\n",
      "7 ----------------------------------\n",
      "DOCID : 196334\n",
      "\n",
      "DATE : March 29, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 12; Column 2 \n",
      "\n",
      "HEADLINE : CHOCOLATE TILES SWEETEN DESSERT TABLE \n",
      "\n",
      "8 ----------------------------------\n",
      "DOCID : 236382\n",
      "\n",
      "DATE : June 21, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : CULINARY SOS: FISHING FOR FROMIN'S SALAD RECIPE \n",
      "\n",
      "9 ----------------------------------\n",
      "DOCID : 265773\n",
      "\n",
      "DATE : August 23, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 18; Column 2 \n",
      "\n",
      "HEADLINE : THE BIRTH OF A PIE RECIPE: IT STARTED WITH A KISS \n",
      "\n",
      "10 ----------------------------------\n",
      "DOCID : 324682\n",
      "\n",
      "DATE : December 20, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 40; Column 1 \n",
      "\n",
      "HEADLINE : SHOPPING: 'TIS THE SEASON TO EAT CHOCOLATE \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "searchAlgorithm = algo.threshold\n",
    "\n",
    "result = searchAlgorithm(query)\n",
    "\n",
    "content_result = documentServer.serveDocuments(result)\n",
    "\n",
    "for idx, doc in enumerate(content_result.keys()):\n",
    "\tprint(idx+1,\"----------------------------------\")\n",
    "\tprint(content_result[doc][\"metadata\"]),\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : the top 10 (doc ID) with the threshold algorithm is 85032, 85141, 321713, 145821, 321712, 111, 196334, 236382, 265773, 324683. It is almost the same than with the two previous algorithms except for the last result 324683 here. This last result is still relevant for the query though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When we look at the consistency of the results computed with the three algorithms they all are similar. The threshold algorithm adds incertainty in the results since it doesn't loop through the whole inverted file and thus may provides better performance (see performance tests below). But because the concern of this first tests is consistency we will now use the fagin algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of stemming, lemmatization and word embedding\n",
    "\n",
    "Setting : \n",
    "    - word score : tf * idf,\n",
    "    - search algorithm : fagin,\n",
    "    - query : \"Chocolate and feet\"\n",
    "\n",
    "Variable parameter : **word processing âˆˆ {<stemming\\>, <lemmatization\\>, <word embedding\\>}**\n",
    "\n",
    "In this section, we compute the top 10 results with the fagin algorithms using an inverted file and a query, both being  built with different word processing techniques.\n",
    "The query for this test is \"Chocolate and feet\", \"feet\" allows us to show the difference between stemming and lemmatization because of its root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't use stemming, lemmatization or word embedding we obtain the following results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyFaginOnQuery(processedQuery):\n",
    "    queryResult = algo.faginAlgo(processedQuery)\n",
    "    if(queryResult):\n",
    "        returnedDocuments = documentServer.serveDocuments(queryResult)\n",
    "        print(\"\\n\")\n",
    "        print(\"results:\\n\")\n",
    "        for idx, doc in enumerate(returnedDocuments.keys()):\n",
    "            print(idx+1,\"----------------------------------\")\n",
    "            print(returnedDocuments[doc][\"metadata\"]),\n",
    "            print(\"----------------------------------\")\n",
    "    else:\n",
    "        print(\"no result\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary set has a size of 290045\n",
      "290045\n",
      "[('chocolate', 3), ('feet', 3)]\n",
      "\n",
      "\n",
      "results:\n",
      "\n",
      "1 ----------------------------------\n",
      "DOCID : 110992\n",
      "\n",
      "DATE : September 22, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Calendar; Part 6; Page 23; Column 1; Entertainment Desk \n",
      "\n",
      "HEADLINE : RESTAURANTS / MAX JACOBSON; \n",
      "</P>\n",
      "<P>\n",
      "CHINESE DINING EXPERIENCE IS EASY ON THE EYE, LESS SO ON PALATE, POCKETBOOK \n",
      "\n",
      "----------------------------------\n",
      "2 ----------------------------------\n",
      "DOCID : 207671\n",
      "\n",
      "DATE : April 22, 1990, Sunday, Home Edition \n",
      "\n",
      "SECTION : Magazine; Page 21; Magazine Desk \n",
      "\n",
      "HEADLINE : THE PLAYGROUND BECOMES THE BATTLEGROUND \n",
      "\n",
      "----------------------------------\n",
      "3 ----------------------------------\n",
      "DOCID : 323491\n",
      "\n",
      "DATE : December 17, 1990, Monday, Valley Edition \n",
      "\n",
      "SECTION : Metro; Part B; Page 3; Column 1 \n",
      "\n",
      "HEADLINE : 2 VOICES CLAIM THEY SPEAK FOR ENCINO; \n",
      "</P>\n",
      "<P>\n",
      "RIVALRY: THE 10-SQUARE-MILE AREA'S PROPERTY OWNERS CAN CHOOSE FROM GROUPS WHOSE \n",
      "APPROACHES TO DEVELOPMENT AND OTHER ISSUES CLASH. MOST PEOPLE DON'T BELONG TO \n",
      "EITHER ONE. \n",
      "\n",
      "----------------------------------\n",
      "4 ----------------------------------\n",
      "DOCID : 74671\n",
      "\n",
      "DATE : June 25, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Part 1; Page 1; Column 1; Metro Desk \n",
      "\n",
      "HEADLINE : CROWDED, TEDIOUS; \n",
      "</P>\n",
      "<P>\n",
      "DEATH ROW: A PLACE FOR FEAR TO GROW \n",
      "\n",
      "----------------------------------\n",
      "5 ----------------------------------\n",
      "DOCID : 247462\n",
      "\n",
      "DATE : July 15, 1990, Sunday, Home Edition \n",
      "\n",
      "SECTION : Part A; Page 1; Column 6; Metro Desk \n",
      "\n",
      "HEADLINE : CREWS WORK TO SHORE UP RAIL TUNNEL; \n",
      "</P>\n",
      "<P>\n",
      "FIRE: ENGINEERS HOPE TO REOPEN DOWNTOWN SECTION OF THE HOLLYWOOD FREEWAY BY \n",
      "MONDAY. THE BLAZE BELOW IS STILL SMOLDERING. \n",
      "\n",
      "----------------------------------\n",
      "6 ----------------------------------\n",
      "DOCID : 30071\n",
      "\n",
      "DATE : March 12, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Travel; Part 7; Page 29; Column 3; Travel Desk \n",
      "\n",
      "HEADLINE : IN LOVE WITH ST. GALLEN \n",
      "\n",
      "----------------------------------\n",
      "7 ----------------------------------\n",
      "DOCID : 53702\n",
      "\n",
      "DATE : May 7, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Long Beach; Part 9; Page 1; Column 2 \n",
      "\n",
      "HEADLINE : SPECIAL YOUNGSTERS SHARE JOY, BEAT OBSTACLES AT TRACK MEET \n",
      "\n",
      "----------------------------------\n",
      "8 ----------------------------------\n",
      "DOCID : 5461\n",
      "\n",
      "DATE : January 15, 1989, Sunday, Home Edition \n",
      "</P>\n",
      "<P>\n",
      "Correction Appended \n",
      "\n",
      "SECTION : Calendar; Page 3; Calendar Desk \n",
      "\n",
      "HEADLINE : SNEAKS '89; \n",
      "</P>\n",
      "<P>\n",
      "HOLLYWOOD, 1989 . . . THE BAD AND THE BEAUTIFUL; \n",
      "</P>\n",
      "<P>\n",
      "ADVENTURES IN THE SCREEN TRADE BRING DEJA VU, WEBS OF INTRIGUE AND VERY HIGH \n",
      "HOPES (OURS AND THEIRS) \n",
      "\n",
      "----------------------------------\n",
      "9 ----------------------------------\n",
      "DOCID : 134434\n",
      "\n",
      "DATE : November 14, 1989, Tuesday, Home Edition \n",
      "\n",
      "SECTION : Part A; Page 1; Column 5; Metro Desk \n",
      "\n",
      "HEADLINE : NEW-FOUND SITE IN JUNGLE MAY BE FIRST MAYAN CITY \n",
      "\n",
      "----------------------------------\n",
      "10 ----------------------------------\n",
      "DOCID : 214793\n",
      "\n",
      "DATE : May 6, 1990, Sunday, Home Edition \n",
      "\n",
      "SECTION : Travel; Part L; Page 14; Column 3; Travel Desk \n",
      "\n",
      "HEADLINE : FOOTLOOSE: GENEVA OFFERS THE BEST OF FRANCE IN SWITZERLAND \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "vocabulary_filename = \"./Globals/nostemm_nolemm_tf_idf/vocabulary.dict\"\n",
    "IF_filename = \"./Globals/nostemm_nolemm_tf_idf/IF.dict\"\n",
    "glob.loadVocabulary(vocabulary_filename,IF_filename)\n",
    "\n",
    "print(\"The vocabulary set has a size of\", len(glob.vocabularyDict))\n",
    "\n",
    "query = \"Chocolate and feet\"\n",
    "\n",
    "# Apply stemming on the query\n",
    "processedQuery = processQueryString(query)\n",
    "print(processedQuery)\n",
    "\n",
    "# Apply fagin algorithm\n",
    "applyFaginOnQuery(processedQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result obtained:\n",
    "\n",
    "    The vocabulary set has a size of  290045\n",
    "\n",
    "    [('chocolate', 3), ('feet', 3)]\n",
    "    \n",
    "    Top 10 : 110992, 207671, 323491, 74671, 247462, 30071, 53702, 5461, 134434, 214793\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add stemming processing on the inverted file and on the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary set has a size of 234118\n",
      "The vocabulary set has a size of 234118\n",
      "[('chocol', 3), ('feet', 3)]\n",
      "\n",
      "\n",
      "results:\n",
      "\n",
      "1 ----------------------------------\n",
      "DOCID : 110992\n",
      "\n",
      "DATE : September 22, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Calendar; Part 6; Page 23; Column 1; Entertainment Desk \n",
      "\n",
      "HEADLINE : RESTAURANTS / MAX JACOBSON; \n",
      "</P>\n",
      "<P>\n",
      "CHINESE DINING EXPERIENCE IS EASY ON THE EYE, LESS SO ON PALATE, POCKETBOOK \n",
      "\n",
      "----------------------------------\n",
      "2 ----------------------------------\n",
      "DOCID : 207671\n",
      "\n",
      "DATE : April 22, 1990, Sunday, Home Edition \n",
      "\n",
      "SECTION : Magazine; Page 21; Magazine Desk \n",
      "\n",
      "HEADLINE : THE PLAYGROUND BECOMES THE BATTLEGROUND \n",
      "\n",
      "----------------------------------\n",
      "3 ----------------------------------\n",
      "DOCID : 323491\n",
      "\n",
      "DATE : December 17, 1990, Monday, Valley Edition \n",
      "\n",
      "SECTION : Metro; Part B; Page 3; Column 1 \n",
      "\n",
      "HEADLINE : 2 VOICES CLAIM THEY SPEAK FOR ENCINO; \n",
      "</P>\n",
      "<P>\n",
      "RIVALRY: THE 10-SQUARE-MILE AREA'S PROPERTY OWNERS CAN CHOOSE FROM GROUPS WHOSE \n",
      "APPROACHES TO DEVELOPMENT AND OTHER ISSUES CLASH. MOST PEOPLE DON'T BELONG TO \n",
      "EITHER ONE. \n",
      "\n",
      "----------------------------------\n",
      "4 ----------------------------------\n",
      "DOCID : 74671\n",
      "\n",
      "DATE : June 25, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Part 1; Page 1; Column 1; Metro Desk \n",
      "\n",
      "HEADLINE : CROWDED, TEDIOUS; \n",
      "</P>\n",
      "<P>\n",
      "DEATH ROW: A PLACE FOR FEAR TO GROW \n",
      "\n",
      "----------------------------------\n",
      "5 ----------------------------------\n",
      "DOCID : 247462\n",
      "\n",
      "DATE : July 15, 1990, Sunday, Home Edition \n",
      "\n",
      "SECTION : Part A; Page 1; Column 6; Metro Desk \n",
      "\n",
      "HEADLINE : CREWS WORK TO SHORE UP RAIL TUNNEL; \n",
      "</P>\n",
      "<P>\n",
      "FIRE: ENGINEERS HOPE TO REOPEN DOWNTOWN SECTION OF THE HOLLYWOOD FREEWAY BY \n",
      "MONDAY. THE BLAZE BELOW IS STILL SMOLDERING. \n",
      "\n",
      "----------------------------------\n",
      "6 ----------------------------------\n",
      "DOCID : 30071\n",
      "\n",
      "DATE : March 12, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Travel; Part 7; Page 29; Column 3; Travel Desk \n",
      "\n",
      "HEADLINE : IN LOVE WITH ST. GALLEN \n",
      "\n",
      "----------------------------------\n",
      "7 ----------------------------------\n",
      "DOCID : 53702\n",
      "\n",
      "DATE : May 7, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Long Beach; Part 9; Page 1; Column 2 \n",
      "\n",
      "HEADLINE : SPECIAL YOUNGSTERS SHARE JOY, BEAT OBSTACLES AT TRACK MEET \n",
      "\n",
      "----------------------------------\n",
      "8 ----------------------------------\n",
      "DOCID : 5461\n",
      "\n",
      "DATE : January 15, 1989, Sunday, Home Edition \n",
      "</P>\n",
      "<P>\n",
      "Correction Appended \n",
      "\n",
      "SECTION : Calendar; Page 3; Calendar Desk \n",
      "\n",
      "HEADLINE : SNEAKS '89; \n",
      "</P>\n",
      "<P>\n",
      "HOLLYWOOD, 1989 . . . THE BAD AND THE BEAUTIFUL; \n",
      "</P>\n",
      "<P>\n",
      "ADVENTURES IN THE SCREEN TRADE BRING DEJA VU, WEBS OF INTRIGUE AND VERY HIGH \n",
      "HOPES (OURS AND THEIRS) \n",
      "\n",
      "----------------------------------\n",
      "9 ----------------------------------\n",
      "DOCID : 103552\n",
      "\n",
      "DATE : September 5, 1989, Tuesday, Orange County Edition \n",
      "\n",
      "SECTION : Business; Part 4; Page 5; Column 1; Financial Desk \n",
      "\n",
      "HEADLINE : RULING THE ROOST; \n",
      "</P>\n",
      "<P>\n",
      "BUSINESS STARTED ON A WING AND A PRAYER NOW FEATHERS HIS NEST \n",
      "\n",
      "----------------------------------\n",
      "10 ----------------------------------\n",
      "DOCID : 134434\n",
      "\n",
      "DATE : November 14, 1989, Tuesday, Home Edition \n",
      "\n",
      "SECTION : Part A; Page 1; Column 5; Metro Desk \n",
      "\n",
      "HEADLINE : NEW-FOUND SITE IN JUNGLE MAY BE FIRST MAYAN CITY \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "vocabulary_filename = \"./Globals/stemm_nolemm_tfidf/vocabulary.dict\"\n",
    "IF_filename = \"./Globals/stemm_nolemm_tfidf/IF.dict\"\n",
    "glob.loadVocabulary(vocabulary_filename,IF_filename)\n",
    "\n",
    "print(\"The vocabulary set has a size of\", len(glob.vocabularyDict))\n",
    "\n",
    "query = \"Chocolate and feet\"\n",
    "\n",
    "# Apply stemming on the query\n",
    "processedQuery = processQueryString(query,stemming = True)\n",
    "print(processedQuery)\n",
    "\n",
    "# Apply fagin algorithm\n",
    "applyFaginOnQuery(processedQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result obtained:\n",
    "\n",
    "    The vocabulary set has a size of  234118\n",
    "\n",
    "    [('chocol', 3), ('feet', 3)]\n",
    "    \n",
    "    Top 10 : 110992, 207671, 323491, 74671, 247462,30071, 53702, 5461, 103552, 134434\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add the lemmatization procedure to tokens in the inverted file and in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary set has a size of 230710\n",
      "The vocabulary set has a size of 230710\n",
      "[('chocol', 3), ('foot', 3)]\n",
      "\n",
      "\n",
      "results:\n",
      "\n",
      "1 ----------------------------------\n",
      "DOCID : 207562\n",
      "\n",
      "DATE : April 21, 1990, Saturday, Orange County Edition \n",
      "\n",
      "SECTION : Orange County Life; Part N; Page 2; Column 1 \n",
      "\n",
      "HEADLINE : HOW MUCH 'I DO' \n",
      "\n",
      "----------------------------------\n",
      "2 ----------------------------------\n",
      "DOCID : 207671\n",
      "\n",
      "DATE : April 22, 1990, Sunday, Home Edition \n",
      "\n",
      "SECTION : Magazine; Page 21; Magazine Desk \n",
      "\n",
      "HEADLINE : THE PLAYGROUND BECOMES THE BATTLEGROUND \n",
      "\n",
      "----------------------------------\n",
      "3 ----------------------------------\n",
      "DOCID : 110992\n",
      "\n",
      "DATE : September 22, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Calendar; Part 6; Page 23; Column 1; Entertainment Desk \n",
      "\n",
      "HEADLINE : RESTAURANTS / MAX JACOBSON; \n",
      "</P>\n",
      "<P>\n",
      "CHINESE DINING EXPERIENCE IS EASY ON THE EYE, LESS SO ON PALATE, POCKETBOOK \n",
      "\n",
      "----------------------------------\n",
      "4 ----------------------------------\n",
      "DOCID : 247462\n",
      "\n",
      "DATE : July 15, 1990, Sunday, Home Edition \n",
      "\n",
      "SECTION : Part A; Page 1; Column 6; Metro Desk \n",
      "\n",
      "HEADLINE : CREWS WORK TO SHORE UP RAIL TUNNEL; \n",
      "</P>\n",
      "<P>\n",
      "FIRE: ENGINEERS HOPE TO REOPEN DOWNTOWN SECTION OF THE HOLLYWOOD FREEWAY BY \n",
      "MONDAY. THE BLAZE BELOW IS STILL SMOLDERING. \n",
      "\n",
      "----------------------------------\n",
      "5 ----------------------------------\n",
      "DOCID : 103552\n",
      "\n",
      "DATE : September 5, 1989, Tuesday, Orange County Edition \n",
      "\n",
      "SECTION : Business; Part 4; Page 5; Column 1; Financial Desk \n",
      "\n",
      "HEADLINE : RULING THE ROOST; \n",
      "</P>\n",
      "<P>\n",
      "BUSINESS STARTED ON A WING AND A PRAYER NOW FEATHERS HIS NEST \n",
      "\n",
      "----------------------------------\n",
      "6 ----------------------------------\n",
      "DOCID : 323491\n",
      "\n",
      "DATE : December 17, 1990, Monday, Valley Edition \n",
      "\n",
      "SECTION : Metro; Part B; Page 3; Column 1 \n",
      "\n",
      "HEADLINE : 2 VOICES CLAIM THEY SPEAK FOR ENCINO; \n",
      "</P>\n",
      "<P>\n",
      "RIVALRY: THE 10-SQUARE-MILE AREA'S PROPERTY OWNERS CAN CHOOSE FROM GROUPS WHOSE \n",
      "APPROACHES TO DEVELOPMENT AND OTHER ISSUES CLASH. MOST PEOPLE DON'T BELONG TO \n",
      "EITHER ONE. \n",
      "\n",
      "----------------------------------\n",
      "7 ----------------------------------\n",
      "DOCID : 134434\n",
      "\n",
      "DATE : November 14, 1989, Tuesday, Home Edition \n",
      "\n",
      "SECTION : Part A; Page 1; Column 5; Metro Desk \n",
      "\n",
      "HEADLINE : NEW-FOUND SITE IN JUNGLE MAY BE FIRST MAYAN CITY \n",
      "\n",
      "----------------------------------\n",
      "8 ----------------------------------\n",
      "DOCID : 295563\n",
      "\n",
      "DATE : October 21, 1990, Sunday, Home Edition \n",
      "\n",
      "SECTION : Travel; Part L; Page 5; Column 1; Travel Desk \n",
      "\n",
      "HEADLINE : ADVENTURE TRAVEL: THE EYES WILL HAVE IT IN BRAZIL'S LUSH PANTANAL \n",
      "\n",
      "----------------------------------\n",
      "9 ----------------------------------\n",
      "DOCID : 74671\n",
      "\n",
      "DATE : June 25, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Part 1; Page 1; Column 1; Metro Desk \n",
      "\n",
      "HEADLINE : CROWDED, TEDIOUS; \n",
      "</P>\n",
      "<P>\n",
      "DEATH ROW: A PLACE FOR FEAR TO GROW \n",
      "\n",
      "----------------------------------\n",
      "10 ----------------------------------\n",
      "DOCID : 30071\n",
      "\n",
      "DATE : March 12, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Travel; Part 7; Page 29; Column 3; Travel Desk \n",
      "\n",
      "HEADLINE : IN LOVE WITH ST. GALLEN \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "vocabulary_filename = \"./Globals/stemm_lemm_tfidf/vocabulary.dict\"\n",
    "IF_filename = \"./Globals/stemm_lemm_tfidf/IF.dict\"\n",
    "glob.loadVocabulary(vocabulary_filename,IF_filename)\n",
    "\n",
    "print(\"The vocabulary set has a size of\", len(glob.vocabularyDict))\n",
    "\n",
    "query = \"Chocolate and feet\"\n",
    "\n",
    "# Apply stemming on the query\n",
    "processedQuery = processQueryString(query,lemmatization = True)\n",
    "print(processedQuery)\n",
    "\n",
    "# Apply fagin algorithm\n",
    "applyFaginOnQuery(processedQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result obtained:\n",
    "\n",
    "    The vocabulary set has a size of  230710\n",
    "\n",
    "    [('chocol', 3), ('foot', 3)]\n",
    "    \n",
    "    Top 10 : 207562, 207671, 110992, 247462, 103552, 323491, 134434, 295563, 74671, 30071\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will extend the query with 3 synonyms for each tokens using word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary set has a size of 230710\n",
      "[('chocol', 3), ('foot', 3), ('caramel', 1), ('eclair', 1), ('cake', 1), ('inch', 1), ('mile', 1), ('diamet', 1)]\n",
      "\n",
      "\n",
      "results:\n",
      "\n",
      "1 ----------------------------------\n",
      "DOCID : 229922\n",
      "\n",
      "DATE : June 7, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 1; Column 3 \n",
      "\n",
      "HEADLINE : WINE COUNTRY CHEFS; \n",
      "</P>\n",
      "<P>\n",
      "THREE YEARS AGO ONLY SIX WINERIES IN NAPA AND SONOMA HAD RESIDENT CHEFS. TODAY \n",
      "THE NUMBER HAS TRIPLED. THESE ARE RISING STARS. \n",
      "\n",
      "----------------------------------\n",
      "2 ----------------------------------\n",
      "DOCID : 141533\n",
      "\n",
      "DATE : November 30, 1989, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 2; Column 1 \n",
      "\n",
      "HEADLINE : EASY-TO-MAKE GIFTS THAT ARE STRAIGHT FROM THE HEART AND THE KITCHEN; \n",
      "</P>\n",
      "<P>\n",
      "HOLIDAYS: FOODS THAT FREEZE WELL OR DON'T REQUIRE REFRIGERATION MAKE THE SAFEST \n",
      "EDIBLE PRESENTS. \n",
      "\n",
      "----------------------------------\n",
      "3 ----------------------------------\n",
      "DOCID : 179442\n",
      "\n",
      "DATE : February 22, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 2; Column 1 \n",
      "\n",
      "HEADLINE : SWEETS, CHOCOLATE AND CALIFORNIANS DOMINATE 34TH PILLSBURY BAKE-OFF; \n",
      "</P>\n",
      "<P>\n",
      "CONTEST: PETALUMA TAX ACCOUNTANT TAKES TOP PRIZE IN PILLSBURY BAKE-OFF. \n",
      "\n",
      "----------------------------------\n",
      "4 ----------------------------------\n",
      "DOCID : 321713\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : GOOD COOKING: MAKE YOUR HOLIDAY INDULGENCE BITTERSWEET \n",
      "\n",
      "----------------------------------\n",
      "5 ----------------------------------\n",
      "DOCID : 132143\n",
      "\n",
      "DATE : November 9, 1989, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 33; Column 4 \n",
      "\n",
      "HEADLINE : RECIPES THAT TAKE YOU BACK TO SCHOOL DAYS \n",
      "\n",
      "----------------------------------\n",
      "6 ----------------------------------\n",
      "DOCID : 209494\n",
      "\n",
      "DATE : April 26, 1990, Thursday, Home Edition \n",
      "</P>\n",
      "<P>\n",
      "Correction Appended \n",
      "\n",
      "SECTION : Food; Part H; Page 1; Column 1 \n",
      "\n",
      "HEADLINE : TEA TIMES; \n",
      "</P>\n",
      "<P>\n",
      "FORGET EVERYTHING YOU EVER KNEW ABOUT THE TRADITIONAL TEA PARTY. TEA HAS GROWN \n",
      "UP, GAINED SOPHISTICATION AND GONE INTERNATIONAL \n",
      "\n",
      "----------------------------------\n",
      "7 ----------------------------------\n",
      "DOCID : 297811\n",
      "\n",
      "DATE : October 25, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 19; Column 1 \n",
      "\n",
      "HEADLINE : YOUR CAKE IS IN THE MAIL; \n",
      "</P>\n",
      "<P>\n",
      "DO-IT-YOURSELF CATALOGUE FOOD \n",
      "\n",
      "----------------------------------\n",
      "8 ----------------------------------\n",
      "DOCID : 148224\n",
      "\n",
      "DATE : December 14, 1989, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 42; Column 1 \n",
      "\n",
      "HEADLINE : COCONUT ADDS A LUSCIOUS TASTE TO FESTIVE CAKES FOR THE HOLIDAY SEASON \n",
      "\n",
      "----------------------------------\n",
      "9 ----------------------------------\n",
      "DOCID : 271941\n",
      "\n",
      "DATE : September 6, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 28; Column 1 \n",
      "\n",
      "HEADLINE : SCHOOL LUNCHES; \n",
      "</P>\n",
      "<P>\n",
      "HOW TO BEAT THE LUNCH BOX BLUES; \n",
      "</P>\n",
      "<P>\n",
      "MENUS: THE LITTLEST FOOD CRITICS -- CATERERS COME UP WITH NEW TWISTS ON OLD \n",
      "FAVORITES IN AN ATTEMPT TO PLEASE THEM. \n",
      "\n",
      "----------------------------------\n",
      "10 ----------------------------------\n",
      "DOCID : 321712\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : BACK TO BASICS: DON'T BE AFRAID: IT'S SIMPLY PERFECT CHOCOLATE \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "vocabulary_filename = \"./Globals/stemm_lemm_tfidf/vocabulary.dict\"\n",
    "IF_filename = \"./Globals/stemm_lemm_tfidf/IF.dict\"\n",
    "glob.loadVocabulary(vocabulary_filename,IF_filename)\n",
    "\n",
    "embeddingFile = open('./Globals/embeddingModel', 'rb')\n",
    "model = pickle.load(embeddingFile)\n",
    "embeddingFile.close()\n",
    "\n",
    "query = \"Chocolate and feet\"\n",
    "\n",
    "# Apply stemming on the query\n",
    "processedQuery = processQueryString(query,lemmatization = True, embedding = True, embeddingModel = model, nbOfSynonyms = 3)\n",
    "print(processedQuery)\n",
    "\n",
    "# Apply fagin algorithm\n",
    "applyFaginOnQuery(processedQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result obtained:\n",
    "\n",
    "    [('chocol', 3), ('foot', 3), ('caramel', 1), ('eclair', 1), ('cake', 1), ('inch', 1), ('mile', 1), ('diamet', 1)]\n",
    "    \n",
    "    Top 10 : 22922, 141533, 179442, 321713, 132143, 209494, 297811, 148224, 271941, 321712\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steming and lemmatization permits to decrease the vocabulary size, especially stemming.**\n",
    "\n",
    "**We can observe that synonyms given by the word embedding model are relevant.**\n",
    "\n",
    "**Stemming doesn't change the results from the fagin algorithms whereas lemmatization changes some documents retrieved.\n",
    "Word embedding permits to extend queries and retrieve new documents.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance tests\n",
    "\n",
    "### 1. Time to build and query the inverted file\n",
    "\n",
    "In this section, we don't use any word processing techniques (stemming, lemmatization, word embedding).\n",
    "\n",
    "Firstly we will build the inverted file over the whole data set in RAM memory then on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ = tokenizer.Tokenizer(datasetFoldername)\n",
    "\n",
    "#cProfile.run(\"ifConstructor.constructIF(tokenizer_, stemming = False, lemmatization = False, wordEmbedding = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/inramprofile.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : cProfile gives us a cpu time of 567 seconds which is roughly equivalent to 10 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/inram13gb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : once the construction is done we can see that the hashmap takes around 1300 MB in RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ = tokenizerCpp.Tokenizer(datasetFoldername, lemmatization_ = False, stemming_ = False)\n",
    "#set runSize such that :\n",
    "#the total number of documents (~130 000) in the dataset divided by runSize is less than the allowed number of \n",
    "#simultaneously opened files on your machine (usually 1024) \n",
    "runSize_ = 10000\n",
    "\n",
    "#cProfile.run(\"ifConstructor.constructIF_diskBased(tokenizer_, runSize = runSize_, score_tf_idf = True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/run10000profile.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : cProfile gives us a cpu time of 1123 seconds which is roughly equivalent to 19 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/run10000.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : from the system monitor we can see that the program reach 583 MB of RAM usage. The upbound is reached during the creation of the temporary files, before the merge operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runSize_ = 130\n",
    "\n",
    "cProfile.run(\"ifConstructor.constructIF_diskBased(tokenizer_, runSize = runSize_, score_tf_idf = True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/run130profile.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : cProfile gives us a cpu time of 5657 seconds which is roughly equivalent to 1h34."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/run130mergemonitor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : from the system monitor we can see that the program reach 137 MB of RAM usage. In this case the upbound is reached during the merge operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The bigger the size of a run the more data will be charged in memory before a flush operation. But, as shown above, the computation time is improved by more RAM usage.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Time to run algorithm\n",
    "In this section, we will use neither stemming/lemmatization nor word embedding. \n",
    "\n",
    "We compute the three algos with same queries of different types:\n",
    "the queries can be not existing in our dictionary, very short, or very long.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to record the test time:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTime(queries):\n",
    "    start_time = time.time()\n",
    "    for query in queries:\n",
    "        algo.naiveAlgo(query)\n",
    "    print(\"--- %s naiveAlgo seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    for query in queries:\n",
    "        algo.faginAlgo(query)\n",
    "    print(\"--- %s faginAlgo seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    for query in queries:\n",
    "        algo.threshold(query)\n",
    "    print(\"--- %s threshold seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.loadVocabulary(\"./Globals/nostemm_nolemm_tf_idf/vocabulary.dict\",\"./Globals/nostemm_nolemm_tf_idf/IF.dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The queries: (the powers are all set to 3 to avoid the effect of word embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneWord = [\n",
    "        [(\"daylight\",3)]\n",
    "        ]\n",
    "\n",
    "notExist = [[(\"fdadfdfewf\",3)],\n",
    "           [(\"114rf4434\",3)],\n",
    "            [(\"jdifjoiq2323\",3)]\n",
    "           ]\n",
    "\n",
    "queries = [\n",
    "                [(\"love\",3), (\"chocolate\",3)],\n",
    "                [(\"january\",3)],\n",
    "                [(\"narrow\",3)],\n",
    "                [(\"today\",3), (\"tomorrow\",3)]           \n",
    "        ]\n",
    "\n",
    "queries1 = [\n",
    "                [(\"love\",3), (\"and\",3), (\"chocolate\",3)],\n",
    "                [(\"january\",3)],\n",
    "                [(\"narrow\",3)],\n",
    "                [(\"today\",3), (\"and\",3), (\"tomorrow\",3)],\n",
    "                [(\"good\",3)],\n",
    "                [(\"better\",3)],\n",
    "                [(\"decent\",3),(\"life\",3), (\"time\",3), (\"evening\",3)]\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the three algos on the words not existing in the dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.124641418457031e-05 naiveAlgo seconds ---\n",
      "--- 0.0005621910095214844 faginAlgo seconds ---\n",
      "--- 0.00039196014404296875 threshold seconds ---\n"
     ]
    }
   ],
   "source": [
    "testTime(notExist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result obtained:\n",
    "\n",
    "--- 4.124641418457031e-05 naiveAlgo seconds ---  \n",
    "--- 0.0005621910095214844 faginAlgo seconds ---  \n",
    "--- 0.00039196014404296875 threshold seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the three algos with one word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0b2cffbf18bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtestTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moneWord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-ff435b785101>\u001b[0m in \u001b[0;36mtestTime\u001b[0;34m(queries)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtestTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaiveAlgo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- %s naiveAlgo seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "testTime(oneWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result obtained: \n",
    "  \n",
    "--- 0.002989053726196289 naiveAlgo seconds ---  \n",
    "--- 0.003451824188232422 faginAlgo seconds ---  \n",
    "--- 0.001968860626220703 threshold seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the three algos with random words\n",
    "Remark: We notice that the fagin algo is quite slow because it needs to go through every posting list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.45010828971862793 naiveAlgo seconds ---\n",
      "--- 8.133760929107666 faginAlgo seconds ---\n",
      "--- 0.44022607803344727 threshold seconds ---\n"
     ]
    }
   ],
   "source": [
    "testTime(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result obtained:\n",
    "\n",
    "\n",
    "--- 0.45010828971862793 naiveAlgo seconds ---  \n",
    "--- 8.133760929107666 faginAlgo seconds ---  \n",
    "--- 0.44022607803344727 threshold seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTime(queries1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result obtained:\n",
    "\n",
    "--- 1.6027929782867432 naiveAlgo seconds ---  \n",
    "--- 92.30611991882324 faginAlgo seconds ---  \n",
    "--- 9.01853895187378 threshold seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "    \n",
    "\n",
    "#### Why naive algo is so rapid at certain cases?\n",
    "\n",
    "we see a very efficient naive algo because in python the sorted function is implemented with binary tree data structure which reduces the time complexity to O(nlogn). The cost of this acceleration is the memory consumed during the calculation.\n",
    "\n",
    "\n",
    "\n",
    "#### Why Fagin algo is so slow in certain cases?\n",
    "\n",
    "Because the fagin algo will continue running if it doesn't find an object which is been listed in all posting lists until it runs through one posting list. In our case, it doesn't exist one doc which has all the terms and the posting list can be very long. That's why the fagin algo can be very slow.\n",
    "\n",
    "\n",
    "#### Comparison of TA and FA:\n",
    "\n",
    "-TA sees less objects than FA and it stops at least as early as FA\n",
    "\n",
    "-TA requires only bounded buffer space while FA makes use of unbouded buffers\n",
    "\n",
    "#### Best Algo?\n",
    "\n",
    "It depends on the aggregation functions characteristics and our database restrictions.\n",
    "In our case, the aggregation function is strictly monotone so we can we can say that TA is the best algo in general cases.  \n",
    "\n",
    "But if the aggregation function changes, or the volume of the database changes, there will be more chances that the FA or TA won't give us a correct results.\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "Remark: the performance test is realized on a machine of 16 GB memory(2400 MHz) of 2.6 GHz 6-Core Intel Core i7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
