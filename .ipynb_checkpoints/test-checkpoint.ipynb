{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests file\n",
    "\n",
    "In this file we will make performance and consistency tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Globals.globals as glob\n",
    "import time\n",
    "import SearchAlgorithms.searchAlgorithms as algo\n",
    "from Tokenization.tokenizer import createListOfTokens, replaceWordsByStem, replaceWordsByLemma, removeStopWords\n",
    "from QueryMaker.queryShell import processQueryString\n",
    "from DocumentServer import documentServer\n",
    "from Tokenization.TokenizationCpp import tokenizer as tokenizerCpp\n",
    "from IFConstruction import ifConstructor\n",
    "\n",
    "documentServer.foldername = \"../latimes\"\n",
    "glob.loadDocID2Content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Impact of the word score on the top 10 documents\n",
    "\n",
    "Setting : \n",
    "    - search algorithm : naive,\n",
    "    - inverted file : no stemming and no lemmatization,\n",
    "    - query processing : no stemming, no lemmatization, no word embedding.\n",
    "    - query : \"Chocolate and internet\"\n",
    "    \n",
    "Variable parameter : **word score âˆˆ {<number of occurence\\>, <tf * idf>}**\n",
    " \n",
    "In this section, we compute the top 10 results with the naive algorithm using an inverted file which has been built without any stemming, lemmatization and no word embedding is applied on the query.\n",
    "We think \"Chocolate and internet\" is a relevant query to test the word score since there is a significant difference between the number of occurence of \"chocolate\" and \"internet\" in the dataset as shown further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chocolate', 3), ('internet', 3)]\n"
     ]
    }
   ],
   "source": [
    "searchAlgorithm = algo.naiveAlgo\n",
    "query = \"Chocolate and internet\"\n",
    "query = processQueryString(query, stemming = False, lemmatization = False, embedding = False)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : because no word embedding is used, the reader must ignore the weights paired with the words. The weights are not used by the naive algorithm anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **First test : word score = number of occurence** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(choco_PL) : 723\n",
      "len(internet_PL) : 4\n",
      "list(choco_PL.items())[:4] : [('321713', 38), ('145821', 27), ('321712', 25), ('111', 24)]\n",
      "list(internet_PL.items())[:4] : [('85032', 8), ('85141', 6), ('105932', 1), ('254071', 1)]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_filename = \"Globals/nostemm_nolemm_notfidf/vocabulary.dict\"\n",
    "IF_filename = \"Globals/nostemm_nolemm_notfidf/IF.dict\"\n",
    "\n",
    "glob.loadVocabulary(vocabulary_filename, IF_filename)\n",
    "\n",
    "choco_PL = glob.voc2PostingList(\"chocolate\")\n",
    "internet_PL = glob.voc2PostingList(\"internet\")\n",
    "\n",
    "print(\"len(choco_PL) :\", len(choco_PL))\n",
    "print(\"len(internet_PL) :\", len(internet_PL))\n",
    "\n",
    "print(\"list(choco_PL.items())[:4] :\", list(choco_PL.items())[:4])\n",
    "print(\"list(internet_PL.items())[:4] :\", list(internet_PL.items())[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : we can observe that \"chocolate\" appears in more documents and with a bigger number of occurence in each documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----------------------------------\n",
      "DOCID : 321713\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : GOOD COOKING: MAKE YOUR HOLIDAY INDULGENCE BITTERSWEET \n",
      "\n",
      "2 ----------------------------------\n",
      "DOCID : 145821\n",
      "\n",
      "DATE : December 8, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Orange County Life; Part N; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : SHE FINDS SWEET SUCCESS WITH CHOCOLATES \n",
      "\n",
      "3 ----------------------------------\n",
      "DOCID : 321712\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : BACK TO BASICS: DON'T BE AFRAID: IT'S SIMPLY PERFECT CHOCOLATE \n",
      "\n",
      "4 ----------------------------------\n",
      "DOCID : 111\n",
      "\n",
      "DATE : January 1, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Opinion; Part 5; Page 5; Column 1; Op-Ed Desk \n",
      "\n",
      "HEADLINE : LITTLE CHOCOLATE DOUGHNUTS TELL THE TALE OF THE U.S. TRADE CRISIS \n",
      "\n",
      "5 ----------------------------------\n",
      "DOCID : 196334\n",
      "\n",
      "DATE : March 29, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 12; Column 2 \n",
      "\n",
      "HEADLINE : CHOCOLATE TILES SWEETEN DESSERT TABLE \n",
      "\n",
      "6 ----------------------------------\n",
      "DOCID : 236382\n",
      "\n",
      "DATE : June 21, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : CULINARY SOS: FISHING FOR FROMIN'S SALAD RECIPE \n",
      "\n",
      "7 ----------------------------------\n",
      "DOCID : 265773\n",
      "\n",
      "DATE : August 23, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 18; Column 2 \n",
      "\n",
      "HEADLINE : THE BIRTH OF A PIE RECIPE: IT STARTED WITH A KISS \n",
      "\n",
      "8 ----------------------------------\n",
      "DOCID : 179442\n",
      "\n",
      "DATE : February 22, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 2; Column 1 \n",
      "\n",
      "HEADLINE : SWEETS, CHOCOLATE AND CALIFORNIANS DOMINATE 34TH PILLSBURY BAKE-OFF; \n",
      "</P>\n",
      "<P>\n",
      "CONTEST: PETALUMA TAX ACCOUNTANT TAKES TOP PRIZE IN PILLSBURY BAKE-OFF. \n",
      "\n",
      "9 ----------------------------------\n",
      "DOCID : 324682\n",
      "\n",
      "DATE : December 20, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 40; Column 1 \n",
      "\n",
      "HEADLINE : SHOPPING: 'TIS THE SEASON TO EAT CHOCOLATE \n",
      "\n",
      "10 ----------------------------------\n",
      "DOCID : 233452\n",
      "\n",
      "DATE : June 14, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : SWEET DREAMS \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = searchAlgorithm(query)\n",
    "\n",
    "content_result = documentServer.serveDocuments(result)\n",
    "\n",
    "for idx, doc in enumerate(content_result.keys()):\n",
    "\tprint(idx+1,\"----------------------------------\")\n",
    "\tprint(content_result[doc][\"metadata\"]),\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : if we look at the headlines of the top 10 results we can clearly see that they all are related to \"chocolate\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Second test : word score = tf * idf = (1 + log(number of occurrences)) * log(total number of documents/(1 + length of posting list)))** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(choco_PL) : 724\n",
      "len(internet_PL) : 4\n",
      "list(choco_PL.items())[:4] : [('321713', 24.139), ('145821', 22.36), ('321712', 21.959), ('111', 21.747)]\n",
      "list(internet_PL.items())[:4] : [('85032', 32.037), ('85141', 29.044), ('105932', 10.403), ('254071', 10.403)]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_filename = \"Globals/nostemm_nolemm_tf_idf/vocabulary.dict\"\n",
    "IF_filename = \"Globals/nostemm_nolemm_tf_idf/IF.dict\"\n",
    "\n",
    "glob.loadVocabulary(vocabulary_filename, IF_filename)\n",
    "\n",
    "choco_PL = glob.voc2PostingList(\"chocolate\")\n",
    "internet_PL = glob.voc2PostingList(\"internet\")\n",
    "\n",
    "print(\"len(choco_PL) :\", len(choco_PL))\n",
    "print(\"len(internet_PL) :\", len(internet_PL))\n",
    "\n",
    "print(\"list(choco_PL.items())[:4] :\", list(choco_PL.items())[:4])\n",
    "print(\"list(internet_PL.items())[:4] :\", list(internet_PL.items())[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : we can observe that, even if \"chocolate\" appears in more documents and with a bigger number of occurence in each documents (as seen before), the new score computation makes \"internet\" reach higher scores than \"chocolate\" in some documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----------------------------------\n",
      "DOCID : 85032\n",
      "\n",
      "DATE : July 21, 1989, Friday, Home Edition \n",
      "\n",
      "SECTION : Part 1; Page 14; Column 5; National Desk \n",
      "\n",
      "HEADLINE : COMPUTER NETWORK SEEN AS STILL VULNERABLE TO VIRUSES \n",
      "\n",
      "2 ----------------------------------\n",
      "DOCID : 85141\n",
      "\n",
      "DATE : July 21, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Business; Part 4; Page 3; Column 5; Financial Desk \n",
      "\n",
      "HEADLINE : PLAN SOUGHT TO KEEP 'VIRUSES' FROM A COMPUTER NETWORK \n",
      "\n",
      "3 ----------------------------------\n",
      "DOCID : 321713\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : GOOD COOKING: MAKE YOUR HOLIDAY INDULGENCE BITTERSWEET \n",
      "\n",
      "4 ----------------------------------\n",
      "DOCID : 145821\n",
      "\n",
      "DATE : December 8, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Orange County Life; Part N; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : SHE FINDS SWEET SUCCESS WITH CHOCOLATES \n",
      "\n",
      "5 ----------------------------------\n",
      "DOCID : 321712\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : BACK TO BASICS: DON'T BE AFRAID: IT'S SIMPLY PERFECT CHOCOLATE \n",
      "\n",
      "6 ----------------------------------\n",
      "DOCID : 111\n",
      "\n",
      "DATE : January 1, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Opinion; Part 5; Page 5; Column 1; Op-Ed Desk \n",
      "\n",
      "HEADLINE : LITTLE CHOCOLATE DOUGHNUTS TELL THE TALE OF THE U.S. TRADE CRISIS \n",
      "\n",
      "7 ----------------------------------\n",
      "DOCID : 196334\n",
      "\n",
      "DATE : March 29, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 12; Column 2 \n",
      "\n",
      "HEADLINE : CHOCOLATE TILES SWEETEN DESSERT TABLE \n",
      "\n",
      "8 ----------------------------------\n",
      "DOCID : 236382\n",
      "\n",
      "DATE : June 21, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : CULINARY SOS: FISHING FOR FROMIN'S SALAD RECIPE \n",
      "\n",
      "9 ----------------------------------\n",
      "DOCID : 265773\n",
      "\n",
      "DATE : August 23, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 18; Column 2 \n",
      "\n",
      "HEADLINE : THE BIRTH OF A PIE RECIPE: IT STARTED WITH A KISS \n",
      "\n",
      "10 ----------------------------------\n",
      "DOCID : 179442\n",
      "\n",
      "DATE : February 22, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 2; Column 1 \n",
      "\n",
      "HEADLINE : SWEETS, CHOCOLATE AND CALIFORNIANS DOMINATE 34TH PILLSBURY BAKE-OFF; \n",
      "</P>\n",
      "<P>\n",
      "CONTEST: PETALUMA TAX ACCOUNTANT TAKES TOP PRIZE IN PILLSBURY BAKE-OFF. \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = searchAlgorithm(query)\n",
    "\n",
    "content_result = documentServer.serveDocuments(result)\n",
    "\n",
    "for idx, doc in enumerate(content_result.keys()):\n",
    "\tprint(idx+1,\"----------------------------------\")\n",
    "\tprint(content_result[doc][\"metadata\"]),\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : now, if we look at the headlines of the top 10 results, both \"chocolate\" and \"internet\" seem to be represented in the results. However, it is not obvious why documents related to \"internet\" should be better than the ones related to \"chocolate\". This behavior is due to the naive algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The score tf * idf shows itself more relevant than a simple word occurence counter since it allows rarer words to be considered by the algorithm and it lower the importance of common words. From now on, our tests will only use this score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Impact of the search algorithm on the top 10 documents\n",
    "\n",
    "In this section, we won't use neither steming/lemmatization nor word embedding. The tf/idf has been choosen as the token score.\n",
    "We also use the query \"Chocolate and internet\" for each algorithm.\n",
    "\n",
    "Firsty, the naive algorithm has been runed previously.\n",
    "The results was :\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "\n",
    "We compute the same query with the fagin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the same query with the threshold algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ????? algorithm seems to be the best.\n",
    "\n",
    "### Impact of stemming, lemmatization and word embedding\n",
    "\n",
    "In this section, we will use the fagin algorithm (TAKE THE BEST) with tf/idf scores on the same query as before : \"Chocolate and internet\".\n",
    "\n",
    "If we don't use stemming, lemmatization or word embedding we obtain the same results as before:\n",
    "DETAILS THE RESULTS\n",
    "\n",
    "We will now add stemming processing on the inverted file and on the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add the lemmatization procedure to tokens in the inverted file and in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will extend the query with 3 synonyms for each tokens using word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION ON STEM LEM EMBEDDING\n",
    "\n",
    "## Performance tests\n",
    "\n",
    "### 1. Time to build and query the inverted file\n",
    "\n",
    "In this section, we will use neither stemming/lemmatization nor word embedding.\n",
    "\n",
    "Firstly we will build the inverted file over the whole data set in RAM memory and resquest it for one posting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will build the inverted file in memory and request one posting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizerCpp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5a6d0f151f5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizerCpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetFoldername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemmatization_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstemming_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#set runSize such that :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#the total number of documents in the dataset divided by runSize is less than the allowed number of simultaneously opened files on your machine (usually 1024)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mifConstructor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstructIF_diskBased\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_tf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizerCpp' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer_ = tokenizerCpp.Tokenizer(datasetFoldername, lemmatization_ = False, stemming_ = True)\n",
    "#set runSize such that :\n",
    "#the total number of documents (~130 000) in the dataset divided by runSize is less than the allowed number of \n",
    "#simultaneously opened files on your machine (usually 1024) \n",
    "ifConstructor.constructIF_diskBased(tokenizer_, runSize = 10000, score_tf_idf = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION\n",
    "\n",
    "### Time to run algorithm\n",
    "\n",
    "In this section, we will use neither stemming/lemmatization nor word embedding. We will also use the query \"Chocolate and internet\" for all algorithm.\n",
    "\n",
    "We compute the naive algorithm on this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the fagin algorithm on the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION\n",
    "\n",
    "### Time to run algorithm\n",
    "\n",
    "In this section, we will use neither stemming/lemmatization nor word embedding. We will also use the query \"Chocolate and internet\" for all algorithm.\n",
    "\n",
    "We compute the naive algorithm on this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the fagin algorithm on the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTime(queries):\n",
    "    start_time = time.time()\n",
    "    for query in queries:\n",
    "        algo.naiveAlgo(query)\n",
    "    print(\"--- %s naiveAlgo seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    for query in queries:\n",
    "        algo.faginAlgo(query)\n",
    "    print(\"--- %s faginAlgo seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    for query in queries:\n",
    "        algo.threshold(query)\n",
    "    print(\"--- %s threshold seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.loadVocabulary(\"./Globals/nostemm_nolemm_tf_idf/vocabulary.dict\",\"./Globals/nostemm_nolemm_tf_idf/IF.dict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneWord = [\n",
    "        [(\"daylight\",3)]\n",
    "    ]\n",
    "\n",
    "notExist = [[(\"fdadfdfewf\",3)],\n",
    "           [(\"114rf4434\",3)],\n",
    "            [(\"jdifjoiq2323\",3)]\n",
    "           ]\n",
    "\n",
    "queries = [\n",
    "                [(\"love\",3), (\"chocolate\",3)],\n",
    "                [(\"january\",3)],\n",
    "                [(\"narrow\",3)],\n",
    "                [(\"today\",3), (\"tomorrow\",3)]           \n",
    "    ]\n",
    "\n",
    "queries1 = [\n",
    "      [(\"love\",3), (\"and\",3), (\"chocolate\",3)],\n",
    "                [(\"january\",3)],\n",
    "                [(\"narrow\",3)],\n",
    "                [(\"today\",3), (\"and\",3), (\"tomorrow\",3)],\n",
    "\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the three algos on the words not existing in the dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.124641418457031e-05 naiveAlgo seconds ---\n",
      "--- 0.0005621910095214844 faginAlgo seconds ---\n",
      "--- 0.00039196014404296875 threshold seconds ---\n"
     ]
    }
   ],
   "source": [
    "testTime(notExist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result obtained:\n",
    "\n",
    "--- 4.124641418457031e-05 naiveAlgo seconds ---  \n",
    "--- 0.0005621910095214844 faginAlgo seconds ---  \n",
    "--- 0.00039196014404296875 threshold seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the three algos with one word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testTime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0b2cffbf18bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtestTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moneWord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'testTime' is not defined"
     ]
    }
   ],
   "source": [
    "testTime(oneWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- 0.002106189727783203 naiveAlgo seconds ---\n",
    "--- 0.004480123519897461 faginAlgo seconds ---\n",
    "--- 0.0021691322326660156 threshold seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the three algos with random words\n",
    "Remark: We notice that the fagin algo is quite slow because it needs to go through every posting list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.45010828971862793 naiveAlgo seconds ---\n",
      "--- 8.133760929107666 faginAlgo seconds ---\n",
      "--- 0.44022607803344727 threshold seconds ---\n"
     ]
    }
   ],
   "source": [
    "testTime(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- 0.45010828971862793 naiveAlgo seconds ---  \n",
    "--- 8.133760929107666 faginAlgo seconds ---  \n",
    "--- 0.44022607803344727 threshold seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
