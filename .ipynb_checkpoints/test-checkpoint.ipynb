{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests file\n",
    "\n",
    "In this file we will make performance and consistency tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!conda install --yes --prefix {sys.prefix} -c conda-forge gensim\n",
    "\n",
    "import Globals.globals as glob\n",
    "import time\n",
    "import SearchAlgorithms.searchAlgorithms as algo\n",
    "from QueryMaker.queryShell import processQueryString\n",
    "from DocumentServer import documentServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of the word score on the top 10 documents\n",
    "\n",
    "In this section, we cumpute the naive algorithm with an inverted file without any stemming, lemmatization or word embedding.\n",
    "The following user query is used in both case : \"Chocolate and internet\".\n",
    "\n",
    "Firstly, the word score is simply the number of instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the same test but with tf/idf as the word score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score tf/idf seems to be better than simply the number of instance. TO CHANGE !!!\n",
    "\n",
    "### Impact of the search algorithm on the top 10 documents\n",
    "\n",
    "In this section, we won't use neither steming/lemmatization nor word embedding. The tf/idf has been choosen as the token score.\n",
    "We also use the query \"Chocolate and internet\" for each algorithm.\n",
    "\n",
    "Firsty, the naive algorithm has been runed previously.\n",
    "The results was :\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "\n",
    "We compute the same query with the fagin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the same query with the threshold algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ????? algorithm seems to be the best.\n",
    "\n",
    "### Impact of stemming, lemmatization and word embedding\n",
    "\n",
    "In this section, we will use the fagin algorithm with tf/idf scores on the query : \"Chocolate and feet\".\n",
    "\n",
    "If we don't use stemming, lemmatization or word embedding we obtain the same results as before:\n",
    "DETAILS THE RESULTS\n",
    "\n",
    "We will now add stemming processing on the inverted file and on the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyFaginOnQuery(processedQuery):\n",
    "    queryResult = algo.faginAlgo(processedQuery)\n",
    "    if(queryResult):\n",
    "        returnedDocuments = documentServer.serveDocuments(queryResult)\n",
    "        print(\"\\n\")\n",
    "        print(\"results:\\n\")\n",
    "        for idx, doc in enumerate(returnedDocuments.keys()):\n",
    "            print(idx+1,\"----------------------------------\")\n",
    "            print(returnedDocuments[doc][\"metadata\"]),\n",
    "            print(\"----------------------------------\")\n",
    "    else:\n",
    "        print(\"no result\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chocol', 3), ('feet', 3)]\n",
      "2\n",
      "[('110992', 47.751000000000005)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005), ('103552', 32.817)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005), ('103552', 32.817), ('30071', 35.199000000000005)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005), ('103552', 32.817), ('30071', 35.199000000000005), ('134434', 32.817)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005), ('103552', 32.817), ('30071', 35.199000000000005), ('134434', 32.817), ('323491', 38.556000000000004)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005), ('103552', 32.817), ('30071', 35.199000000000005), ('134434', 32.817), ('323491', 38.556000000000004), ('53702', 35.199000000000005)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005), ('103552', 32.817), ('30071', 35.199000000000005), ('134434', 32.817), ('323491', 38.556000000000004), ('53702', 35.199000000000005), ('5461', 35.199000000000005)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005), ('103552', 32.817), ('30071', 35.199000000000005), ('134434', 32.817), ('323491', 38.556000000000004), ('53702', 35.199000000000005), ('5461', 35.199000000000005), ('74671', 38.556000000000004)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005), ('103552', 32.817), ('30071', 35.199000000000005), ('134434', 32.817), ('323491', 38.556000000000004), ('53702', 35.199000000000005), ('5461', 35.199000000000005), ('74671', 38.556000000000004), ('207671', 43.521)]\n",
      "\n",
      "\n",
      "results:\n",
      "\n",
      "1 ----------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-69b3e996f5e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Apply fagin algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mapplyFaginOnQuery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessedQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-89-fa13d980e92d>\u001b[0m in \u001b[0;36mapplyFaginOnQuery\u001b[1;34m(processedQuery)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturnedDocuments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"----------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturnedDocuments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"metadata\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"----------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             '''if(returnedDocuments):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "glob.loadVocabulary(\"./Globals/stemm_nolemm_tfidf/vocabulary.dict\",\"./Globals/stemm_nolemm_tfidf/IF.dict\")\n",
    "\n",
    "query = \"Chocolate and feet\"\n",
    "\n",
    "# Apply stemming on the query\n",
    "processedQuery = processQueryString(query,stemming = True)\n",
    "print(processedQuery)\n",
    "\n",
    "# Apply fagin algorithm\n",
    "applyFaginOnQuery(processedQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result obtained:\n",
    "\n",
    "    The vocabulary set has a size of  234118\n",
    "\n",
    "    [('chocol', 3), ('feet', 3)]\n",
    "    \n",
    "    Top 10 :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add the lemmatization procedure to tokens in the inverted file and in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not open/read file: ./Globals/stemm_lemm_tfidf/vocabulary.dict\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "glob.loadVocabulary(\"./Globals/stemm_lemm_tfidf/vocabulary.dict\",\"./Globals/stemm_lemm_tfidf/IF.dict\")\n",
    "\n",
    "query = \"Chocolate and feet\"\n",
    "\n",
    "# Apply stemming on the query\n",
    "processedQuery = processQueryString(query,lemmatization = True)\n",
    "print(processedQuery)\n",
    "\n",
    "# Apply fagin algorithm\n",
    "applyFaginOnQuery(processedQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will extend the query with 3 synonyms for each tokens using word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not open/read file: ./Globals/stemm_lemm_tfidf/vocabulary.dict\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "glob.loadVocabulary(\"./Globals/stemm_lemm_tfidf/vocabulary.dict\",\"./Globals/stemm_lemm_tfidf/IF.dict\")\n",
    "\n",
    "embeddingFile = open('./Globals/embeddingModel', 'rb')\n",
    "model = pickle.load(embeddingFile)\n",
    "embeddingFile.close()\n",
    "\n",
    "query = \"Chocolate and feet\"\n",
    "\n",
    "# Apply stemming on the query\n",
    "processedQuery = processQueryString(query,lemmatization = True, embedding = True, embeddingModel = model, nbOfSynonyms = 3)\n",
    "print(processedQuery)\n",
    "\n",
    "# Apply fagin algorithm\n",
    "applyFaginOnQuery(processedQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION ON STEM LEM EMBEDDING\n",
    "\n",
    "## Performance tests\n",
    "\n",
    "### Time to build the inverted file\n",
    "\n",
    "In this section, we will use neither stemming/lemmatization nor word embedding.\n",
    "\n",
    "Firstly we will build the inverted file over the whole data set in RAM memory and resquest it for one posting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will build the inverted file in memory and request one posting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTime(queries):\n",
    "    start_time = time.time()\n",
    "    for query in queries:\n",
    "        algo.naiveAlgo(query)\n",
    "    print(\"--- %s naiveAlgo seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    for query in queries:\n",
    "        algo.faginAlgo(query)\n",
    "    print(\"--- %s faginAlgo seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    for query in queries:\n",
    "        algo.threshold(query)\n",
    "    print(\"--- %s threshold seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.loadVocabulary(\"./Globals/nostemm_nolemm_tf_idf/vocabulary.dict\",\"./Globals/nostemm_nolemm_tf_idf/IF.dict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneWord = [\n",
    "        [(\"daylight\",3)]\n",
    "    ]\n",
    "\n",
    "notExist = [[(\"fdadfdfewf\",3)],\n",
    "           [(\"114rf4434\",3)],\n",
    "            [(\"jdifjoiq2323\",3)]\n",
    "           ]\n",
    "\n",
    "queries = [\n",
    "                [(\"love\",3), (\"chocolate\",3)],\n",
    "                [(\"january\",3)],\n",
    "                [(\"narrow\",3)],\n",
    "                [(\"today\",3), (\"tomorrow\",3)]           \n",
    "    ]\n",
    "\n",
    "queries1 = [\n",
    "      [(\"love\",3), (\"and\",3), (\"chocolate\",3)],\n",
    "                [(\"january\",3)],\n",
    "                [(\"narrow\",3)],\n",
    "                [(\"today\",3), (\"and\",3), (\"tomorrow\",3)],\n",
    "\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the three algos on the words not existing in the dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.124641418457031e-05 naiveAlgo seconds ---\n",
      "--- 0.0005621910095214844 faginAlgo seconds ---\n",
      "--- 0.00039196014404296875 threshold seconds ---\n"
     ]
    }
   ],
   "source": [
    "testTime(notExist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- 4.124641418457031e-05 naiveAlgo seconds ---  \n",
    "--- 0.0005621910095214844 faginAlgo seconds ---  \n",
    "--- 0.00039196014404296875 threshold seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the three algos with one word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.002106189727783203 naiveAlgo seconds ---\n",
      "--- 0.004480123519897461 faginAlgo seconds ---\n",
      "--- 0.0021691322326660156 threshold seconds ---\n"
     ]
    }
   ],
   "source": [
    "testTime(oneWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- 0.002106189727783203 naiveAlgo seconds ---  \n",
    "--- 0.004480123519897461 faginAlgo seconds ---  \n",
    "--- 0.0021691322326660156 threshold seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the three algos with random words\n",
    "Remark: We notice that the fagin algo is quite slow because it needs to go through every posting list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.45010828971862793 naiveAlgo seconds ---\n",
      "--- 8.133760929107666 faginAlgo seconds ---\n",
      "--- 0.44022607803344727 threshold seconds ---\n"
     ]
    }
   ],
   "source": [
    "testTime(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- 0.45010828971862793 naiveAlgo seconds ---  \n",
    "--- 8.133760929107666 faginAlgo seconds ---  \n",
    "--- 0.44022607803344727 threshold seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
