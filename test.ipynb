{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests file\n",
    "\n",
    "In this file we will make performance and consistency tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!conda install --yes --prefix {sys.prefix} -c conda-forge gensim\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import Globals.globals as glob\n",
    "import SearchAlgorithms.searchAlgorithms as algo\n",
    "from Tokenization.tokenizer import createListOfTokens, replaceWordsByStem, replaceWordsByLemma, removeStopWords\n",
    "from QueryMaker.queryShell import processQueryString\n",
    "from DocumentServer import documentServer\n",
    "from Tokenization.TokenizationCpp import tokenizer as tokenizerCpp\n",
    "from IFConstruction import ifConstructor\n",
    "\n",
    "datasetFoldername = \"../latimes\"\n",
    "documentServer.foldername = datasetFoldername\n",
    "glob.loadDocID2Content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Impact of the word score on the top 10 documents\n",
    "\n",
    "Setting : \n",
    "    - search algorithm : naive,\n",
    "    - inverted file : no stemming and no lemmatization,\n",
    "    - query processing : no stemming, no lemmatization, no word embedding.\n",
    "    - query : \"Chocolate and internet\"\n",
    "    \n",
    "Variable parameter : **word score âˆˆ {<number of occurence\\>, <tf * idf>}**\n",
    " \n",
    "In this section, we compute the top 10 results with the naive algorithm using an inverted file which has been built without any stemming, lemmatization and no word embedding is applied on the query.\n",
    "We think \"Chocolate and internet\" is a relevant query to test the word score since there is a significant difference between the number of occurence of \"chocolate\" and \"internet\" in the dataset as shown further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chocolate', 3), ('internet', 3)]\n"
     ]
    }
   ],
   "source": [
    "searchAlgorithm = algo.naiveAlgo\n",
    "query = \"Chocolate and internet\"\n",
    "query = processQueryString(query, stemming = False, lemmatization = False, embedding = False)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : because no word embedding is used, the reader must ignore the weights paired with the words. The weights are not used by the naive algorithm anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **First test : word score = number of occurence** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(choco_PL) : 723\n",
      "len(internet_PL) : 4\n",
      "list(choco_PL.items())[:4] : [('321713', 38), ('145821', 27), ('321712', 25), ('111', 24)]\n",
      "list(internet_PL.items())[:4] : [('85032', 8), ('85141', 6), ('105932', 1), ('254071', 1)]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_filename = \"Globals/nostemm_nolemm_notfidf/vocabulary.dict\"\n",
    "IF_filename = \"Globals/nostemm_nolemm_notfidf/IF.dict\"\n",
    "\n",
    "glob.loadVocabulary(vocabulary_filename, IF_filename)\n",
    "\n",
    "choco_PL = glob.voc2PostingList(\"chocolate\")\n",
    "internet_PL = glob.voc2PostingList(\"internet\")\n",
    "\n",
    "print(\"len(choco_PL) :\", len(choco_PL))\n",
    "print(\"len(internet_PL) :\", len(internet_PL))\n",
    "\n",
    "print(\"list(choco_PL.items())[:4] :\", list(choco_PL.items())[:4])\n",
    "print(\"list(internet_PL.items())[:4] :\", list(internet_PL.items())[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : we can observe that \"chocolate\" appears in more documents and with a bigger number of occurence in each documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----------------------------------\n",
      "DOCID : 321713\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : GOOD COOKING: MAKE YOUR HOLIDAY INDULGENCE BITTERSWEET \n",
      "\n",
      "2 ----------------------------------\n",
      "DOCID : 145821\n",
      "\n",
      "DATE : December 8, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Orange County Life; Part N; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : SHE FINDS SWEET SUCCESS WITH CHOCOLATES \n",
      "\n",
      "3 ----------------------------------\n",
      "DOCID : 321712\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : BACK TO BASICS: DON'T BE AFRAID: IT'S SIMPLY PERFECT CHOCOLATE \n",
      "\n",
      "4 ----------------------------------\n",
      "DOCID : 111\n",
      "\n",
      "DATE : January 1, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Opinion; Part 5; Page 5; Column 1; Op-Ed Desk \n",
      "\n",
      "HEADLINE : LITTLE CHOCOLATE DOUGHNUTS TELL THE TALE OF THE U.S. TRADE CRISIS \n",
      "\n",
      "5 ----------------------------------\n",
      "DOCID : 196334\n",
      "\n",
      "DATE : March 29, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 12; Column 2 \n",
      "\n",
      "HEADLINE : CHOCOLATE TILES SWEETEN DESSERT TABLE \n",
      "\n",
      "6 ----------------------------------\n",
      "DOCID : 236382\n",
      "\n",
      "DATE : June 21, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : CULINARY SOS: FISHING FOR FROMIN'S SALAD RECIPE \n",
      "\n",
      "7 ----------------------------------\n",
      "DOCID : 265773\n",
      "\n",
      "DATE : August 23, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 18; Column 2 \n",
      "\n",
      "HEADLINE : THE BIRTH OF A PIE RECIPE: IT STARTED WITH A KISS \n",
      "\n",
      "8 ----------------------------------\n",
      "DOCID : 179442\n",
      "\n",
      "DATE : February 22, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 2; Column 1 \n",
      "\n",
      "HEADLINE : SWEETS, CHOCOLATE AND CALIFORNIANS DOMINATE 34TH PILLSBURY BAKE-OFF; \n",
      "</P>\n",
      "<P>\n",
      "CONTEST: PETALUMA TAX ACCOUNTANT TAKES TOP PRIZE IN PILLSBURY BAKE-OFF. \n",
      "\n",
      "9 ----------------------------------\n",
      "DOCID : 324682\n",
      "\n",
      "DATE : December 20, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 40; Column 1 \n",
      "\n",
      "HEADLINE : SHOPPING: 'TIS THE SEASON TO EAT CHOCOLATE \n",
      "\n",
      "10 ----------------------------------\n",
      "DOCID : 233452\n",
      "\n",
      "DATE : June 14, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : SWEET DREAMS \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = searchAlgorithm(query)\n",
    "\n",
    "content_result = documentServer.serveDocuments(result)\n",
    "\n",
    "for idx, doc in enumerate(content_result.keys()):\n",
    "\tprint(idx+1,\"----------------------------------\")\n",
    "\tprint(content_result[doc][\"metadata\"]),\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : if we look at the headlines of the top 10 results we can clearly see that they all are related to \"chocolate\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Second test : word score = tf * idf = (1 + log(number of occurrences)) * log(total number of documents/(1 + length of posting list)))** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(choco_PL) : 724\n",
      "len(internet_PL) : 4\n",
      "list(choco_PL.items())[:4] : [('321713', 24.139), ('145821', 22.36), ('321712', 21.959), ('111', 21.747)]\n",
      "list(internet_PL.items())[:4] : [('85032', 32.037), ('85141', 29.044), ('105932', 10.403), ('254071', 10.403)]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_filename = \"Globals/nostemm_nolemm_tf_idf/vocabulary.dict\"\n",
    "IF_filename = \"Globals/nostemm_nolemm_tf_idf/IF.dict\"\n",
    "\n",
    "glob.loadVocabulary(vocabulary_filename, IF_filename)\n",
    "\n",
    "choco_PL = glob.voc2PostingList(\"chocolate\")\n",
    "internet_PL = glob.voc2PostingList(\"internet\")\n",
    "\n",
    "print(\"len(choco_PL) :\", len(choco_PL))\n",
    "print(\"len(internet_PL) :\", len(internet_PL))\n",
    "\n",
    "print(\"list(choco_PL.items())[:4] :\", list(choco_PL.items())[:4])\n",
    "print(\"list(internet_PL.items())[:4] :\", list(internet_PL.items())[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : we can observe that, even if \"chocolate\" appears in more documents and with a bigger number of occurence in each documents (as seen before), the new score computation makes \"internet\" reach higher scores than \"chocolate\" in some documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----------------------------------\n",
      "DOCID : 85032\n",
      "\n",
      "DATE : July 21, 1989, Friday, Home Edition \n",
      "\n",
      "SECTION : Part 1; Page 14; Column 5; National Desk \n",
      "\n",
      "HEADLINE : COMPUTER NETWORK SEEN AS STILL VULNERABLE TO VIRUSES \n",
      "\n",
      "2 ----------------------------------\n",
      "DOCID : 85141\n",
      "\n",
      "DATE : July 21, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Business; Part 4; Page 3; Column 5; Financial Desk \n",
      "\n",
      "HEADLINE : PLAN SOUGHT TO KEEP 'VIRUSES' FROM A COMPUTER NETWORK \n",
      "\n",
      "3 ----------------------------------\n",
      "DOCID : 321713\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : GOOD COOKING: MAKE YOUR HOLIDAY INDULGENCE BITTERSWEET \n",
      "\n",
      "4 ----------------------------------\n",
      "DOCID : 145821\n",
      "\n",
      "DATE : December 8, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Orange County Life; Part N; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : SHE FINDS SWEET SUCCESS WITH CHOCOLATES \n",
      "\n",
      "5 ----------------------------------\n",
      "DOCID : 321712\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : BACK TO BASICS: DON'T BE AFRAID: IT'S SIMPLY PERFECT CHOCOLATE \n",
      "\n",
      "6 ----------------------------------\n",
      "DOCID : 111\n",
      "\n",
      "DATE : January 1, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Opinion; Part 5; Page 5; Column 1; Op-Ed Desk \n",
      "\n",
      "HEADLINE : LITTLE CHOCOLATE DOUGHNUTS TELL THE TALE OF THE U.S. TRADE CRISIS \n",
      "\n",
      "7 ----------------------------------\n",
      "DOCID : 196334\n",
      "\n",
      "DATE : March 29, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 12; Column 2 \n",
      "\n",
      "HEADLINE : CHOCOLATE TILES SWEETEN DESSERT TABLE \n",
      "\n",
      "8 ----------------------------------\n",
      "DOCID : 236382\n",
      "\n",
      "DATE : June 21, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : CULINARY SOS: FISHING FOR FROMIN'S SALAD RECIPE \n",
      "\n",
      "9 ----------------------------------\n",
      "DOCID : 265773\n",
      "\n",
      "DATE : August 23, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 18; Column 2 \n",
      "\n",
      "HEADLINE : THE BIRTH OF A PIE RECIPE: IT STARTED WITH A KISS \n",
      "\n",
      "10 ----------------------------------\n",
      "DOCID : 179442\n",
      "\n",
      "DATE : February 22, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 2; Column 1 \n",
      "\n",
      "HEADLINE : SWEETS, CHOCOLATE AND CALIFORNIANS DOMINATE 34TH PILLSBURY BAKE-OFF; \n",
      "</P>\n",
      "<P>\n",
      "CONTEST: PETALUMA TAX ACCOUNTANT TAKES TOP PRIZE IN PILLSBURY BAKE-OFF. \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = searchAlgorithm(query)\n",
    "\n",
    "content_result = documentServer.serveDocuments(result)\n",
    "\n",
    "for idx, doc in enumerate(content_result.keys()):\n",
    "\tprint(idx+1,\"----------------------------------\")\n",
    "\tprint(content_result[doc][\"metadata\"]),\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : now, if we look at the headlines of the top 10 results, both \"chocolate\" and \"internet\" seem to be represented in the results. However, it is not obvious why documents related to \"internet\" should be better than the ones related to \"chocolate\". This behavior is due to the naive algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The score tf * idf shows itself more relevant than a simple word occurence counter since it allows rarer words to be considered by the algorithm and it lower the importance of common words. From now on, our tests will only use this score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Impact of the search algorithm on the top 10 documents\n",
    "\n",
    "In this section, we won't use neither steming/lemmatization nor word embedding. The tf/idf has been choosen as the token score.\n",
    "We also use the query \"Chocolate and internet\" for each algorithm.\n",
    "\n",
    "Firsty, the naive algorithm has been runed previously.\n",
    "The results was :\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "\n",
    "We compute the same query with the fagin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the same query with the threshold algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ????? algorithm seems to be the best.\n",
    "\n",
    "### Impact of stemming, lemmatization and word embedding\n",
    "\n",
    "In this section, we will use the fagin algorithm with tf/idf scores on the query : \"Chocolate and feet\".\n",
    "\n",
    "If we don't use stemming, lemmatization or word embedding we obtain the same results as before:\n",
    "DETAILS THE RESULTS\n",
    "\n",
    "We will now add stemming processing on the inverted file and on the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyFaginOnQuery(processedQuery):\n",
    "    queryResult = algo.faginAlgo(processedQuery)\n",
    "    if(queryResult):\n",
    "        returnedDocuments = documentServer.serveDocuments(queryResult)\n",
    "        print(\"\\n\")\n",
    "        print(\"results:\\n\")\n",
    "        for idx, doc in enumerate(returnedDocuments.keys()):\n",
    "            print(idx+1,\"----------------------------------\")\n",
    "            print(returnedDocuments[doc][\"metadata\"]),\n",
    "            print(\"----------------------------------\")\n",
    "    else:\n",
    "        print(\"no result\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chocol', 3), ('feet', 3)]\n",
      "2\n",
      "[('110992', 47.751000000000005)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005), ('103552', 32.817)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005), ('103552', 32.817), ('30071', 35.199000000000005)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005), ('103552', 32.817), ('30071', 35.199000000000005), ('134434', 32.817)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005), ('103552', 32.817), ('30071', 35.199000000000005), ('134434', 32.817), ('323491', 38.556000000000004)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005), ('103552', 32.817), ('30071', 35.199000000000005), ('134434', 32.817), ('323491', 38.556000000000004), ('53702', 35.199000000000005)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005), ('103552', 32.817), ('30071', 35.199000000000005), ('134434', 32.817), ('323491', 38.556000000000004), ('53702', 35.199000000000005), ('5461', 35.199000000000005)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005), ('103552', 32.817), ('30071', 35.199000000000005), ('134434', 32.817), ('323491', 38.556000000000004), ('53702', 35.199000000000005), ('5461', 35.199000000000005), ('74671', 38.556000000000004)]\n",
      "[('110992', 47.751000000000005), ('247462', 35.199000000000005), ('103552', 32.817), ('30071', 35.199000000000005), ('134434', 32.817), ('323491', 38.556000000000004), ('53702', 35.199000000000005), ('5461', 35.199000000000005), ('74671', 38.556000000000004), ('207671', 43.521)]\n",
      "\n",
      "\n",
      "results:\n",
      "\n",
      "1 ----------------------------------\n",
      "DOCID : 110992\n",
      "\n",
      "DATE : September 22, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Calendar; Part 6; Page 23; Column 1; Entertainment Desk \n",
      "\n",
      "HEADLINE : RESTAURANTS / MAX JACOBSON; \n",
      "</P>\n",
      "<P>\n",
      "CHINESE DINING EXPERIENCE IS EASY ON THE EYE, LESS SO ON PALATE, POCKETBOOK \n",
      "\n",
      "----------------------------------\n",
      "2 ----------------------------------\n",
      "DOCID : 207671\n",
      "\n",
      "DATE : April 22, 1990, Sunday, Home Edition \n",
      "\n",
      "SECTION : Magazine; Page 21; Magazine Desk \n",
      "\n",
      "HEADLINE : THE PLAYGROUND BECOMES THE BATTLEGROUND \n",
      "\n",
      "----------------------------------\n",
      "3 ----------------------------------\n",
      "DOCID : 323491\n",
      "\n",
      "DATE : December 17, 1990, Monday, Valley Edition \n",
      "\n",
      "SECTION : Metro; Part B; Page 3; Column 1 \n",
      "\n",
      "HEADLINE : 2 VOICES CLAIM THEY SPEAK FOR ENCINO; \n",
      "</P>\n",
      "<P>\n",
      "RIVALRY: THE 10-SQUARE-MILE AREA'S PROPERTY OWNERS CAN CHOOSE FROM GROUPS WHOSE \n",
      "APPROACHES TO DEVELOPMENT AND OTHER ISSUES CLASH. MOST PEOPLE DON'T BELONG TO \n",
      "EITHER ONE. \n",
      "\n",
      "----------------------------------\n",
      "4 ----------------------------------\n",
      "DOCID : 74671\n",
      "\n",
      "DATE : June 25, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Part 1; Page 1; Column 1; Metro Desk \n",
      "\n",
      "HEADLINE : CROWDED, TEDIOUS; \n",
      "</P>\n",
      "<P>\n",
      "DEATH ROW: A PLACE FOR FEAR TO GROW \n",
      "\n",
      "----------------------------------\n",
      "5 ----------------------------------\n",
      "DOCID : 247462\n",
      "\n",
      "DATE : July 15, 1990, Sunday, Home Edition \n",
      "\n",
      "SECTION : Part A; Page 1; Column 6; Metro Desk \n",
      "\n",
      "HEADLINE : CREWS WORK TO SHORE UP RAIL TUNNEL; \n",
      "</P>\n",
      "<P>\n",
      "FIRE: ENGINEERS HOPE TO REOPEN DOWNTOWN SECTION OF THE HOLLYWOOD FREEWAY BY \n",
      "MONDAY. THE BLAZE BELOW IS STILL SMOLDERING. \n",
      "\n",
      "----------------------------------\n",
      "6 ----------------------------------\n",
      "DOCID : 30071\n",
      "\n",
      "DATE : March 12, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Travel; Part 7; Page 29; Column 3; Travel Desk \n",
      "\n",
      "HEADLINE : IN LOVE WITH ST. GALLEN \n",
      "\n",
      "----------------------------------\n",
      "7 ----------------------------------\n",
      "DOCID : 53702\n",
      "\n",
      "DATE : May 7, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Long Beach; Part 9; Page 1; Column 2 \n",
      "\n",
      "HEADLINE : SPECIAL YOUNGSTERS SHARE JOY, BEAT OBSTACLES AT TRACK MEET \n",
      "\n",
      "----------------------------------\n",
      "8 ----------------------------------\n",
      "DOCID : 5461\n",
      "\n",
      "DATE : January 15, 1989, Sunday, Home Edition \n",
      "</P>\n",
      "<P>\n",
      "Correction Appended \n",
      "\n",
      "SECTION : Calendar; Page 3; Calendar Desk \n",
      "\n",
      "HEADLINE : SNEAKS '89; \n",
      "</P>\n",
      "<P>\n",
      "HOLLYWOOD, 1989 . . . THE BAD AND THE BEAUTIFUL; \n",
      "</P>\n",
      "<P>\n",
      "ADVENTURES IN THE SCREEN TRADE BRING DEJA VU, WEBS OF INTRIGUE AND VERY HIGH \n",
      "HOPES (OURS AND THEIRS) \n",
      "\n",
      "----------------------------------\n",
      "9 ----------------------------------\n",
      "DOCID : 103552\n",
      "\n",
      "DATE : September 5, 1989, Tuesday, Orange County Edition \n",
      "\n",
      "SECTION : Business; Part 4; Page 5; Column 1; Financial Desk \n",
      "\n",
      "HEADLINE : RULING THE ROOST; \n",
      "</P>\n",
      "<P>\n",
      "BUSINESS STARTED ON A WING AND A PRAYER NOW FEATHERS HIS NEST \n",
      "\n",
      "----------------------------------\n",
      "10 ----------------------------------\n",
      "DOCID : 134434\n",
      "\n",
      "DATE : November 14, 1989, Tuesday, Home Edition \n",
      "\n",
      "SECTION : Part A; Page 1; Column 5; Metro Desk \n",
      "\n",
      "HEADLINE : NEW-FOUND SITE IN JUNGLE MAY BE FIRST MAYAN CITY \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "glob.loadVocabulary(\"./Globals/stemm_nolemm_tfidf/vocabulary.dict\",\"./Globals/stemm_nolemm_tfidf/IF.dict\")\n",
    "\n",
    "query = \"Chocolate and feet\"\n",
    "\n",
    "# Apply stemming on the query\n",
    "processedQuery = processQueryString(query,stemming = True)\n",
    "print(processedQuery)\n",
    "\n",
    "# Apply fagin algorithm\n",
    "applyFaginOnQuery(processedQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result obtained:\n",
    "\n",
    "    The vocabulary set has a size of  234118\n",
    "\n",
    "    [('chocol', 3), ('feet', 3)]\n",
    "    \n",
    "    Top 10 :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add the lemmatization procedure to tokens in the inverted file and in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chocol', 3), ('foot', 3)]\n",
      "2\n",
      "[('103552', 33.873000000000005)]\n",
      "[('103552', 33.873000000000005), ('207671', 44.583000000000006)]\n",
      "[('103552', 33.873000000000005), ('207671', 44.583000000000006), ('134434', 32.910000000000004)]\n",
      "[('103552', 33.873000000000005), ('207671', 44.583000000000006), ('134434', 32.910000000000004), ('247462', 36.105000000000004)]\n",
      "[('103552', 33.873000000000005), ('207671', 44.583000000000006), ('134434', 32.910000000000004), ('247462', 36.105000000000004), ('110992', 42.480000000000004)]\n",
      "[('103552', 33.873000000000005), ('207671', 44.583000000000006), ('134434', 32.910000000000004), ('247462', 36.105000000000004), ('110992', 42.480000000000004), ('295563', 32.910000000000004)]\n",
      "[('103552', 33.873000000000005), ('207671', 44.583000000000006), ('134434', 32.910000000000004), ('247462', 36.105000000000004), ('110992', 42.480000000000004), ('295563', 32.910000000000004), ('207562', 48.744)]\n",
      "[('103552', 33.873000000000005), ('207671', 44.583000000000006), ('134434', 32.910000000000004), ('247462', 36.105000000000004), ('110992', 42.480000000000004), ('295563', 32.910000000000004), ('207562', 48.744), ('323491', 33.873000000000005)]\n",
      "[('103552', 33.873000000000005), ('207671', 44.583000000000006), ('134434', 32.910000000000004), ('247462', 36.105000000000004), ('110992', 42.480000000000004), ('295563', 32.910000000000004), ('207562', 48.744), ('323491', 33.873000000000005), ('74671', 32.910000000000004)]\n",
      "[('103552', 33.873000000000005), ('207671', 44.583000000000006), ('134434', 32.910000000000004), ('247462', 36.105000000000004), ('110992', 42.480000000000004), ('295563', 32.910000000000004), ('207562', 48.744), ('323491', 33.873000000000005), ('74671', 32.910000000000004), ('30071', 31.770000000000003)]\n",
      "\n",
      "\n",
      "results:\n",
      "\n",
      "1 ----------------------------------\n",
      "DOCID : 207562\n",
      "\n",
      "DATE : April 21, 1990, Saturday, Orange County Edition \n",
      "\n",
      "SECTION : Orange County Life; Part N; Page 2; Column 1 \n",
      "\n",
      "HEADLINE : HOW MUCH 'I DO' \n",
      "\n",
      "----------------------------------\n",
      "2 ----------------------------------\n",
      "DOCID : 207671\n",
      "\n",
      "DATE : April 22, 1990, Sunday, Home Edition \n",
      "\n",
      "SECTION : Magazine; Page 21; Magazine Desk \n",
      "\n",
      "HEADLINE : THE PLAYGROUND BECOMES THE BATTLEGROUND \n",
      "\n",
      "----------------------------------\n",
      "3 ----------------------------------\n",
      "DOCID : 110992\n",
      "\n",
      "DATE : September 22, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Calendar; Part 6; Page 23; Column 1; Entertainment Desk \n",
      "\n",
      "HEADLINE : RESTAURANTS / MAX JACOBSON; \n",
      "</P>\n",
      "<P>\n",
      "CHINESE DINING EXPERIENCE IS EASY ON THE EYE, LESS SO ON PALATE, POCKETBOOK \n",
      "\n",
      "----------------------------------\n",
      "4 ----------------------------------\n",
      "DOCID : 247462\n",
      "\n",
      "DATE : July 15, 1990, Sunday, Home Edition \n",
      "\n",
      "SECTION : Part A; Page 1; Column 6; Metro Desk \n",
      "\n",
      "HEADLINE : CREWS WORK TO SHORE UP RAIL TUNNEL; \n",
      "</P>\n",
      "<P>\n",
      "FIRE: ENGINEERS HOPE TO REOPEN DOWNTOWN SECTION OF THE HOLLYWOOD FREEWAY BY \n",
      "MONDAY. THE BLAZE BELOW IS STILL SMOLDERING. \n",
      "\n",
      "----------------------------------\n",
      "5 ----------------------------------\n",
      "DOCID : 103552\n",
      "\n",
      "DATE : September 5, 1989, Tuesday, Orange County Edition \n",
      "\n",
      "SECTION : Business; Part 4; Page 5; Column 1; Financial Desk \n",
      "\n",
      "HEADLINE : RULING THE ROOST; \n",
      "</P>\n",
      "<P>\n",
      "BUSINESS STARTED ON A WING AND A PRAYER NOW FEATHERS HIS NEST \n",
      "\n",
      "----------------------------------\n",
      "6 ----------------------------------\n",
      "DOCID : 323491\n",
      "\n",
      "DATE : December 17, 1990, Monday, Valley Edition \n",
      "\n",
      "SECTION : Metro; Part B; Page 3; Column 1 \n",
      "\n",
      "HEADLINE : 2 VOICES CLAIM THEY SPEAK FOR ENCINO; \n",
      "</P>\n",
      "<P>\n",
      "RIVALRY: THE 10-SQUARE-MILE AREA'S PROPERTY OWNERS CAN CHOOSE FROM GROUPS WHOSE \n",
      "APPROACHES TO DEVELOPMENT AND OTHER ISSUES CLASH. MOST PEOPLE DON'T BELONG TO \n",
      "EITHER ONE. \n",
      "\n",
      "----------------------------------\n",
      "7 ----------------------------------\n",
      "DOCID : 134434\n",
      "\n",
      "DATE : November 14, 1989, Tuesday, Home Edition \n",
      "\n",
      "SECTION : Part A; Page 1; Column 5; Metro Desk \n",
      "\n",
      "HEADLINE : NEW-FOUND SITE IN JUNGLE MAY BE FIRST MAYAN CITY \n",
      "\n",
      "----------------------------------\n",
      "8 ----------------------------------\n",
      "DOCID : 295563\n",
      "\n",
      "DATE : October 21, 1990, Sunday, Home Edition \n",
      "\n",
      "SECTION : Travel; Part L; Page 5; Column 1; Travel Desk \n",
      "\n",
      "HEADLINE : ADVENTURE TRAVEL: THE EYES WILL HAVE IT IN BRAZIL'S LUSH PANTANAL \n",
      "\n",
      "----------------------------------\n",
      "9 ----------------------------------\n",
      "DOCID : 74671\n",
      "\n",
      "DATE : June 25, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Part 1; Page 1; Column 1; Metro Desk \n",
      "\n",
      "HEADLINE : CROWDED, TEDIOUS; \n",
      "</P>\n",
      "<P>\n",
      "DEATH ROW: A PLACE FOR FEAR TO GROW \n",
      "\n",
      "----------------------------------\n",
      "10 ----------------------------------\n",
      "DOCID : 30071\n",
      "\n",
      "DATE : March 12, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Travel; Part 7; Page 29; Column 3; Travel Desk \n",
      "\n",
      "HEADLINE : IN LOVE WITH ST. GALLEN \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "glob.loadVocabulary(\"./Globals/stemm_lemm_tfidf/vocabulary.dict\",\"./Globals/stemm_lemm_tfidf/IF.dict\")\n",
    "\n",
    "query = \"Chocolate and feet\"\n",
    "\n",
    "# Apply stemming on the query\n",
    "processedQuery = processQueryString(query,lemmatization = True)\n",
    "print(processedQuery)\n",
    "\n",
    "# Apply fagin algorithm\n",
    "applyFaginOnQuery(processedQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will extend the query with 3 synonyms for each tokens using word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chocol', 3), ('foot', 3), ('caramel', 1), ('eclair', 1), ('cake', 1), ('inch', 1), ('mile', 1), ('diamet', 1)]\n",
      "8\n",
      "\n",
      "\n",
      "results:\n",
      "\n",
      "1 ----------------------------------\n",
      "DOCID : 229922\n",
      "\n",
      "DATE : June 7, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 1; Column 3 \n",
      "\n",
      "HEADLINE : WINE COUNTRY CHEFS; \n",
      "</P>\n",
      "<P>\n",
      "THREE YEARS AGO ONLY SIX WINERIES IN NAPA AND SONOMA HAD RESIDENT CHEFS. TODAY \n",
      "THE NUMBER HAS TRIPLED. THESE ARE RISING STARS. \n",
      "\n",
      "----------------------------------\n",
      "2 ----------------------------------\n",
      "DOCID : 141533\n",
      "\n",
      "DATE : November 30, 1989, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 2; Column 1 \n",
      "\n",
      "HEADLINE : EASY-TO-MAKE GIFTS THAT ARE STRAIGHT FROM THE HEART AND THE KITCHEN; \n",
      "</P>\n",
      "<P>\n",
      "HOLIDAYS: FOODS THAT FREEZE WELL OR DON'T REQUIRE REFRIGERATION MAKE THE SAFEST \n",
      "EDIBLE PRESENTS. \n",
      "\n",
      "----------------------------------\n",
      "3 ----------------------------------\n",
      "DOCID : 179442\n",
      "\n",
      "DATE : February 22, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 2; Column 1 \n",
      "\n",
      "HEADLINE : SWEETS, CHOCOLATE AND CALIFORNIANS DOMINATE 34TH PILLSBURY BAKE-OFF; \n",
      "</P>\n",
      "<P>\n",
      "CONTEST: PETALUMA TAX ACCOUNTANT TAKES TOP PRIZE IN PILLSBURY BAKE-OFF. \n",
      "\n",
      "----------------------------------\n",
      "4 ----------------------------------\n",
      "DOCID : 321713\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : GOOD COOKING: MAKE YOUR HOLIDAY INDULGENCE BITTERSWEET \n",
      "\n",
      "----------------------------------\n",
      "5 ----------------------------------\n",
      "DOCID : 132143\n",
      "\n",
      "DATE : November 9, 1989, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 33; Column 4 \n",
      "\n",
      "HEADLINE : RECIPES THAT TAKE YOU BACK TO SCHOOL DAYS \n",
      "\n",
      "----------------------------------\n",
      "6 ----------------------------------\n",
      "DOCID : 209494\n",
      "\n",
      "DATE : April 26, 1990, Thursday, Home Edition \n",
      "</P>\n",
      "<P>\n",
      "Correction Appended \n",
      "\n",
      "SECTION : Food; Part H; Page 1; Column 1 \n",
      "\n",
      "HEADLINE : TEA TIMES; \n",
      "</P>\n",
      "<P>\n",
      "FORGET EVERYTHING YOU EVER KNEW ABOUT THE TRADITIONAL TEA PARTY. TEA HAS GROWN \n",
      "UP, GAINED SOPHISTICATION AND GONE INTERNATIONAL \n",
      "\n",
      "----------------------------------\n",
      "7 ----------------------------------\n",
      "DOCID : 297811\n",
      "\n",
      "DATE : October 25, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 19; Column 1 \n",
      "\n",
      "HEADLINE : YOUR CAKE IS IN THE MAIL; \n",
      "</P>\n",
      "<P>\n",
      "DO-IT-YOURSELF CATALOGUE FOOD \n",
      "\n",
      "----------------------------------\n",
      "8 ----------------------------------\n",
      "DOCID : 148224\n",
      "\n",
      "DATE : December 14, 1989, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 42; Column 1 \n",
      "\n",
      "HEADLINE : COCONUT ADDS A LUSCIOUS TASTE TO FESTIVE CAKES FOR THE HOLIDAY SEASON \n",
      "\n",
      "----------------------------------\n",
      "9 ----------------------------------\n",
      "DOCID : 271941\n",
      "\n",
      "DATE : September 6, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 28; Column 1 \n",
      "\n",
      "HEADLINE : SCHOOL LUNCHES; \n",
      "</P>\n",
      "<P>\n",
      "HOW TO BEAT THE LUNCH BOX BLUES; \n",
      "</P>\n",
      "<P>\n",
      "MENUS: THE LITTLEST FOOD CRITICS -- CATERERS COME UP WITH NEW TWISTS ON OLD \n",
      "FAVORITES IN AN ATTEMPT TO PLEASE THEM. \n",
      "\n",
      "----------------------------------\n",
      "10 ----------------------------------\n",
      "DOCID : 321712\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : BACK TO BASICS: DON'T BE AFRAID: IT'S SIMPLY PERFECT CHOCOLATE \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "glob.loadVocabulary(\"./Globals/stemm_lemm_tfidf/vocabulary.dict\",\"./Globals/stemm_lemm_tfidf/IF.dict\")\n",
    "\n",
    "embeddingFile = open('./Globals/embeddingModel', 'rb')\n",
    "model = pickle.load(embeddingFile)\n",
    "embeddingFile.close()\n",
    "\n",
    "query = \"Chocolate and feet\"\n",
    "\n",
    "# Apply stemming on the query\n",
    "processedQuery = processQueryString(query,lemmatization = True, embedding = True, embeddingModel = model, nbOfSynonyms = 3)\n",
    "print(processedQuery)\n",
    "\n",
    "# Apply fagin algorithm\n",
    "applyFaginOnQuery(processedQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION ON STEM LEM EMBEDDING\n",
    "\n",
    "## Performance tests\n",
    "\n",
    "### 1. Time to build and query the inverted file\n",
    "\n",
    "In this section, we will use neither stemming/lemmatization nor word embedding.\n",
    "\n",
    "Firstly we will build the inverted file over the whole data set in RAM memory and resquest it for one posting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will build the inverted file in memory and request one posting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ = tokenizerCpp.Tokenizer(datasetFoldername, lemmatization_ = False, stemming_ = False)\n",
    "#set runSize such that :\n",
    "#the total number of documents (~130 000) in the dataset divided by runSize is less than the allowed number of \n",
    "#simultaneously opened files on your machine (usually 1024) \n",
    "runSize_ = 10000\n",
    "\n",
    "#ifConstructor.constructIF_diskBased(tokenizer_, runSize = runSize_, score_tf_idf = True)\n",
    "cProfile.run(\"ifConstructor.constructIF_diskBased(tokenizer_, runSize = runSize_, score_tf_idf = True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/run10000profile.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : cProfile gives us a cpu time of 1123 seconds which is roughly equivalent to 19 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/run10000.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : from the system monitor we can see that the program reach 583 MB of RAM usage.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runSize_ = 130\n",
    "\n",
    "cProfile.run(\"ifConstructor.constructIF_diskBased(tokenizer_, runSize = runSize_, score_tf_idf = True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION\n",
    "\n",
    "### Time to run algorithm\n",
    "\n",
    "In this section, we will use neither stemming/lemmatization nor word embedding. We will also use the query \"Chocolate and internet\" for all algorithm.\n",
    "\n",
    "We compute the naive algorithm on this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the fagin algorithm on the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION\n",
    "\n",
    "### Time to run algorithm\n",
    "\n",
    "In this section, we will use neither stemming/lemmatization nor word embedding. We will also use the query \"Chocolate and internet\" for all algorithm.\n",
    "\n",
    "We compute the naive algorithm on this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the fagin algorithm on the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTime(queries):\n",
    "    start_time = time.time()\n",
    "    for query in queries:\n",
    "        algo.naiveAlgo(query)\n",
    "    print(\"--- %s naiveAlgo seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    for query in queries:\n",
    "        algo.faginAlgo(query)\n",
    "    print(\"--- %s faginAlgo seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    for query in queries:\n",
    "        algo.threshold(query)\n",
    "    print(\"--- %s threshold seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.loadVocabulary(\"./Globals/nostemm_nolemm_tf_idf/vocabulary.dict\",\"./Globals/nostemm_nolemm_tf_idf/IF.dict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneWord = [\n",
    "        [(\"daylight\",3)]\n",
    "    ]\n",
    "\n",
    "notExist = [[(\"fdadfdfewf\",3)],\n",
    "           [(\"114rf4434\",3)],\n",
    "            [(\"jdifjoiq2323\",3)]\n",
    "           ]\n",
    "\n",
    "queries = [\n",
    "                [(\"love\",3), (\"chocolate\",3)],\n",
    "                [(\"january\",3)],\n",
    "                [(\"narrow\",3)],\n",
    "                [(\"today\",3), (\"tomorrow\",3)]           \n",
    "    ]\n",
    "\n",
    "queries1 = [\n",
    "      [(\"love\",3), (\"and\",3), (\"chocolate\",3)],\n",
    "                [(\"january\",3)],\n",
    "                [(\"narrow\",3)],\n",
    "                [(\"today\",3), (\"and\",3), (\"tomorrow\",3)],\n",
    "\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the three algos on the words not existing in the dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.124641418457031e-05 naiveAlgo seconds ---\n",
      "--- 0.0005621910095214844 faginAlgo seconds ---\n",
      "--- 0.00039196014404296875 threshold seconds ---\n"
     ]
    }
   ],
   "source": [
    "testTime(notExist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result obtained:\n",
    "\n",
    "--- 4.124641418457031e-05 naiveAlgo seconds ---  \n",
    "--- 0.0005621910095214844 faginAlgo seconds ---  \n",
    "--- 0.00039196014404296875 threshold seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the three algos with one word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testTime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0b2cffbf18bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtestTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moneWord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'testTime' is not defined"
     ]
    }
   ],
   "source": [
    "testTime(oneWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- 0.002106189727783203 naiveAlgo seconds ---\n",
    "--- 0.004480123519897461 faginAlgo seconds ---\n",
    "--- 0.0021691322326660156 threshold seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the three algos with random words\n",
    "Remark: We notice that the fagin algo is quite slow because it needs to go through every posting list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.45010828971862793 naiveAlgo seconds ---\n",
      "--- 8.133760929107666 faginAlgo seconds ---\n",
      "--- 0.44022607803344727 threshold seconds ---\n"
     ]
    }
   ],
   "source": [
    "testTime(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- 0.45010828971862793 naiveAlgo seconds ---  \n",
    "--- 8.133760929107666 faginAlgo seconds ---  \n",
    "--- 0.44022607803344727 threshold seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
