{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests file\n",
    "\n",
    "In this file we will make performance and consistency tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Globals.globals as glob\n",
    "import time\n",
    "import SearchAlgorithms.searchAlgorithms as algo\n",
    "from Tokenization.tokenizer import createListOfTokens, replaceWordsByStem, replaceWordsByLemma, removeStopWords\n",
    "from QueryMaker.queryShell import processQueryString\n",
    "from DocumentServer import documentServer\n",
    "from Tokenization.TokenizationCpp import tokenizer as tokenizerCpp\n",
    "from IFConstruction import ifConstructor\n",
    "\n",
    "datasetFoldername = \"../latimes\"\n",
    "documentServer.foldername = datasetFoldername\n",
    "glob.loadDocID2Content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistency tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Impact of the word score on the top 10 documents\n",
    "\n",
    "Setting : \n",
    "    - search algorithm : naive,\n",
    "    - inverted file : no stemming and no lemmatization,\n",
    "    - query processing : no stemming, no lemmatization, no word embedding.\n",
    "    - query : \"Chocolate and internet\"\n",
    "    \n",
    "Variable parameter : **word score âˆˆ {<number of occurence\\>, <tf * idf>}**\n",
    " \n",
    "In this section, we compute the top 10 results with the naive algorithm using an inverted file which has been built without any stemming, lemmatization and no word embedding is applied on the query.\n",
    "We think \"Chocolate and internet\" is a relevant query to test the word score since there is a significant difference between the number of occurence of \"chocolate\" and \"internet\" in the dataset as shown further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chocolate', 3), ('internet', 3)]\n"
     ]
    }
   ],
   "source": [
    "searchAlgorithm = algo.naiveAlgo\n",
    "query = \"Chocolate and internet\"\n",
    "query = processQueryString(query, stemming = False, lemmatization = False, embedding = False)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : because no word embedding is used, the reader must ignore the weights paired with the words. The weights are not used by the naive algorithm anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **First test : word score = number of occurence** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(choco_PL) : 723\n",
      "len(internet_PL) : 4\n",
      "list(choco_PL.items())[:4] : [('321713', 38), ('145821', 27), ('321712', 25), ('111', 24)]\n",
      "list(internet_PL.items())[:4] : [('85032', 8), ('85141', 6), ('105932', 1), ('254071', 1)]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_filename = \"Globals/nostemm_nolemm_notfidf/vocabulary.dict\"\n",
    "IF_filename = \"Globals/nostemm_nolemm_notfidf/IF.dict\"\n",
    "\n",
    "glob.loadVocabulary(vocabulary_filename, IF_filename)\n",
    "\n",
    "choco_PL = glob.voc2PostingList(\"chocolate\")\n",
    "internet_PL = glob.voc2PostingList(\"internet\")\n",
    "\n",
    "print(\"len(choco_PL) :\", len(choco_PL))\n",
    "print(\"len(internet_PL) :\", len(internet_PL))\n",
    "\n",
    "print(\"list(choco_PL.items())[:4] :\", list(choco_PL.items())[:4])\n",
    "print(\"list(internet_PL.items())[:4] :\", list(internet_PL.items())[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : we can observe that \"chocolate\" appears in more documents and with a bigger number of occurence in each documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----------------------------------\n",
      "DOCID : 321713\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : GOOD COOKING: MAKE YOUR HOLIDAY INDULGENCE BITTERSWEET \n",
      "\n",
      "2 ----------------------------------\n",
      "DOCID : 145821\n",
      "\n",
      "DATE : December 8, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Orange County Life; Part N; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : SHE FINDS SWEET SUCCESS WITH CHOCOLATES \n",
      "\n",
      "3 ----------------------------------\n",
      "DOCID : 321712\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : BACK TO BASICS: DON'T BE AFRAID: IT'S SIMPLY PERFECT CHOCOLATE \n",
      "\n",
      "4 ----------------------------------\n",
      "DOCID : 111\n",
      "\n",
      "DATE : January 1, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Opinion; Part 5; Page 5; Column 1; Op-Ed Desk \n",
      "\n",
      "HEADLINE : LITTLE CHOCOLATE DOUGHNUTS TELL THE TALE OF THE U.S. TRADE CRISIS \n",
      "\n",
      "5 ----------------------------------\n",
      "DOCID : 196334\n",
      "\n",
      "DATE : March 29, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 12; Column 2 \n",
      "\n",
      "HEADLINE : CHOCOLATE TILES SWEETEN DESSERT TABLE \n",
      "\n",
      "6 ----------------------------------\n",
      "DOCID : 236382\n",
      "\n",
      "DATE : June 21, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : CULINARY SOS: FISHING FOR FROMIN'S SALAD RECIPE \n",
      "\n",
      "7 ----------------------------------\n",
      "DOCID : 265773\n",
      "\n",
      "DATE : August 23, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 18; Column 2 \n",
      "\n",
      "HEADLINE : THE BIRTH OF A PIE RECIPE: IT STARTED WITH A KISS \n",
      "\n",
      "8 ----------------------------------\n",
      "DOCID : 179442\n",
      "\n",
      "DATE : February 22, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 2; Column 1 \n",
      "\n",
      "HEADLINE : SWEETS, CHOCOLATE AND CALIFORNIANS DOMINATE 34TH PILLSBURY BAKE-OFF; \n",
      "</P>\n",
      "<P>\n",
      "CONTEST: PETALUMA TAX ACCOUNTANT TAKES TOP PRIZE IN PILLSBURY BAKE-OFF. \n",
      "\n",
      "9 ----------------------------------\n",
      "DOCID : 324682\n",
      "\n",
      "DATE : December 20, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 40; Column 1 \n",
      "\n",
      "HEADLINE : SHOPPING: 'TIS THE SEASON TO EAT CHOCOLATE \n",
      "\n",
      "10 ----------------------------------\n",
      "DOCID : 233452\n",
      "\n",
      "DATE : June 14, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : SWEET DREAMS \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = searchAlgorithm(query)\n",
    "\n",
    "content_result = documentServer.serveDocuments(result)\n",
    "\n",
    "for idx, doc in enumerate(content_result.keys()):\n",
    "\tprint(idx+1,\"----------------------------------\")\n",
    "\tprint(content_result[doc][\"metadata\"]),\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : if we look at the headlines of the top 10 results we can clearly see that they all are related to \"chocolate\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Second test : word score = tf * idf = (1 + log(number of occurrences)) * log(total number of documents/(1 + length of posting list)))** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(choco_PL) : 724\n",
      "len(internet_PL) : 4\n",
      "list(choco_PL.items())[:4] : [('321713', 24.139), ('145821', 22.36), ('321712', 21.959), ('111', 21.747)]\n",
      "list(internet_PL.items())[:4] : [('85032', 32.037), ('85141', 29.044), ('105932', 10.403), ('254071', 10.403)]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_filename = \"Globals/nostemm_nolemm_tf_idf/vocabulary.dict\"\n",
    "IF_filename = \"Globals/nostemm_nolemm_tf_idf/IF.dict\"\n",
    "\n",
    "glob.loadVocabulary(vocabulary_filename, IF_filename)\n",
    "\n",
    "choco_PL = glob.voc2PostingList(\"chocolate\")\n",
    "internet_PL = glob.voc2PostingList(\"internet\")\n",
    "\n",
    "print(\"len(choco_PL) :\", len(choco_PL))\n",
    "print(\"len(internet_PL) :\", len(internet_PL))\n",
    "\n",
    "print(\"list(choco_PL.items())[:4] :\", list(choco_PL.items())[:4])\n",
    "print(\"list(internet_PL.items())[:4] :\", list(internet_PL.items())[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : we can observe that, even if \"chocolate\" appears in more documents and with a bigger number of occurence in each documents (as seen before), the new score computation makes \"internet\" reach higher scores than \"chocolate\" in some documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----------------------------------\n",
      "DOCID : 85032\n",
      "\n",
      "DATE : July 21, 1989, Friday, Home Edition \n",
      "\n",
      "SECTION : Part 1; Page 14; Column 5; National Desk \n",
      "\n",
      "HEADLINE : COMPUTER NETWORK SEEN AS STILL VULNERABLE TO VIRUSES \n",
      "\n",
      "2 ----------------------------------\n",
      "DOCID : 85141\n",
      "\n",
      "DATE : July 21, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Business; Part 4; Page 3; Column 5; Financial Desk \n",
      "\n",
      "HEADLINE : PLAN SOUGHT TO KEEP 'VIRUSES' FROM A COMPUTER NETWORK \n",
      "\n",
      "3 ----------------------------------\n",
      "DOCID : 321713\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : GOOD COOKING: MAKE YOUR HOLIDAY INDULGENCE BITTERSWEET \n",
      "\n",
      "4 ----------------------------------\n",
      "DOCID : 145821\n",
      "\n",
      "DATE : December 8, 1989, Friday, Orange County Edition \n",
      "\n",
      "SECTION : Orange County Life; Part N; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : SHE FINDS SWEET SUCCESS WITH CHOCOLATES \n",
      "\n",
      "5 ----------------------------------\n",
      "DOCID : 321712\n",
      "\n",
      "DATE : December 13, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 20; Column 1 \n",
      "\n",
      "HEADLINE : BACK TO BASICS: DON'T BE AFRAID: IT'S SIMPLY PERFECT CHOCOLATE \n",
      "\n",
      "6 ----------------------------------\n",
      "DOCID : 111\n",
      "\n",
      "DATE : January 1, 1989, Sunday, Home Edition \n",
      "\n",
      "SECTION : Opinion; Part 5; Page 5; Column 1; Op-Ed Desk \n",
      "\n",
      "HEADLINE : LITTLE CHOCOLATE DOUGHNUTS TELL THE TALE OF THE U.S. TRADE CRISIS \n",
      "\n",
      "7 ----------------------------------\n",
      "DOCID : 196334\n",
      "\n",
      "DATE : March 29, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 12; Column 2 \n",
      "\n",
      "HEADLINE : CHOCOLATE TILES SWEETEN DESSERT TABLE \n",
      "\n",
      "8 ----------------------------------\n",
      "DOCID : 236382\n",
      "\n",
      "DATE : June 21, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 11; Column 1 \n",
      "\n",
      "HEADLINE : CULINARY SOS: FISHING FOR FROMIN'S SALAD RECIPE \n",
      "\n",
      "9 ----------------------------------\n",
      "DOCID : 265773\n",
      "\n",
      "DATE : August 23, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 18; Column 2 \n",
      "\n",
      "HEADLINE : THE BIRTH OF A PIE RECIPE: IT STARTED WITH A KISS \n",
      "\n",
      "10 ----------------------------------\n",
      "DOCID : 179442\n",
      "\n",
      "DATE : February 22, 1990, Thursday, Home Edition \n",
      "\n",
      "SECTION : Food; Part H; Page 2; Column 1 \n",
      "\n",
      "HEADLINE : SWEETS, CHOCOLATE AND CALIFORNIANS DOMINATE 34TH PILLSBURY BAKE-OFF; \n",
      "</P>\n",
      "<P>\n",
      "CONTEST: PETALUMA TAX ACCOUNTANT TAKES TOP PRIZE IN PILLSBURY BAKE-OFF. \n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = searchAlgorithm(query)\n",
    "\n",
    "content_result = documentServer.serveDocuments(result)\n",
    "\n",
    "for idx, doc in enumerate(content_result.keys()):\n",
    "\tprint(idx+1,\"----------------------------------\")\n",
    "\tprint(content_result[doc][\"metadata\"]),\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : now, if we look at the headlines of the top 10 results, both \"chocolate\" and \"internet\" seem to be represented in the results. However, it is not obvious why documents related to \"internet\" should be better than the ones related to \"chocolate\". This behavior is due to the naive algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The score tf * idf shows itself more relevant than a simple word occurence counter since it allows rarer words to be considered by the algorithm and it lower the importance of common words. From now on, our tests will only use this score.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Impact of the search algorithm on the top 10 documents\n",
    "\n",
    "In this section, we won't use neither steming/lemmatization nor word embedding. The tf/idf has been choosen as the token score.\n",
    "We also use the query \"Chocolate and internet\" for each algorithm.\n",
    "\n",
    "Firsty, the naive algorithm has been runed previously.\n",
    "The results was :\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "....\n",
    "\n",
    "We compute the same query with the fagin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the same query with the threshold algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ????? algorithm seems to be the best.\n",
    "\n",
    "### Impact of stemming, lemmatization and word embedding\n",
    "\n",
    "In this section, we will use the fagin algorithm (TAKE THE BEST) with tf/idf scores on the same query as before : \"Chocolate and internet\".\n",
    "\n",
    "If we don't use stemming, lemmatization or word embedding we obtain the same results as before:\n",
    "DETAILS THE RESULTS\n",
    "\n",
    "We will now add stemming processing on the inverted file and on the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add the lemmatization procedure to tokens in the inverted file and in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will extend the query with 3 synonyms for each tokens using word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION ON STEM LEM EMBEDDING\n",
    "\n",
    "## Performance tests\n",
    "\n",
    "### 1. Time to build and query the inverted file\n",
    "\n",
    "In this section, we will use neither stemming/lemmatization nor word embedding.\n",
    "\n",
    "Firstly we will build the inverted file over the whole data set in RAM memory and resquest it for one posting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will build the inverted file in memory and request one posting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> disk-based inverted file construction with a run size of: 10000\n",
      "> start parsing dataset in triples (word, docID, number of occurence)\n",
      "> rm IFConstruction/tmp/*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 99 % Doc number : 131148 % Doc number : 10268 7 % Doc number : 10418 8 % Doc number : 10576Doc number : 10873 8 % Doc number : 11038 8 % Doc number : 11218 11372 8 % Doc number : 11528 % Doc number : 11842 9 % Doc number : 12467Doc number : 12618 9 % Doc number : 12773 10 % Doc number : 13236 10 % Doc number : 13379 Doc number : 13529 10 % Doc number : 13678 10 % Doc number : 13983Doc number : 14292 11 % Doc number : 14762 11 % Doc number : 15078 12 % Doc number : 15908 12 % Doc number : 16220 12 % Doc number : 16525 12 % Doc number : 16917 13 % Doc number : 17957 Doc number : 18112 13 % Doc number : 18428 14 % Doc number : 18660 14 % Doc number : 18782 14 % Doc number : 1925015 % Doc number : 1992919954 15 % Doc number : 20104 15 % Doc number : 20391 15 % Doc number : 20547 16 % Doc number : 21108 16 % Doc number : 21264 16 % Doc number : 22034 16 % Doc number : 22340 Doc number : 22487 17 % Doc number : 22805 17 % Doc number : 22962 17 % Doc number : 23093 17 % Doc number : 23566 17 % Doc number : 23724Doc number : 23877 18 % Doc number : 24183 18 % Doc number : 24331 Doc number : 24787 18 % Doc number : 2494425326 19 % Doc number : 25457 19 % Doc number : 25900 19 % Doc number : 26055Doc number : 26186 19 % Doc number : 26336 20 % Doc number : 26946 20 % Doc number : 2726027429 20 % Doc number : 27562 % Doc number : 27993 21 % Doc number : 28680% Doc number : 28824 22 % Doc number : 29122 22 % Doc number : 29503 22 % Doc number : 2979630278 23 % Doc number : 30600% Doc number : 30770 23 % Doc number : 30946 23 % Doc number : 31096 23 % Doc number : 31563 24 % Doc number : 32488 24 % Doc number : 32632 24 % Doc number : 32770 25 % Doc number : 332073334333492 25 % Doc number : 34240 26 % Doc number : 34405 % Doc number : 34716 % Doc number : 34864 26 % Doc number : 35186 26 % Doc number : 35522 27 % Doc number : 35674 27 % Doc number : 35957 27 % Doc number : 36101 27 % Doc number : 36425 27 % Doc number : 36746 27 % Doc number : 36906 28 % Doc number : 37059 28 % Doc number : 37223 28 % Doc number : 37361 29 % Doc number : 38650 29 % Doc number : 39407 29 % Doc number : 39566 % Doc number : 40113 30 % Doc number : 40219 30 % Doc number : 4036831 % Doc number : 40943 31 % Doc number : 41099 31 % Doc number : 41229 31 % Doc number : 41636 31 % Doc number : 41902 31 % Doc number : 42050 31 % Doc number : 42199 32 % Doc number : 42362 % Doc number : 42639 32 % Doc number : 42917 32 % Doc number : 43033 32 % Doc number : 43411 33 % Doc number : 43815 33 % Doc number : 44086 33 % Doc number : 44227 33 % Doc number : 44641 % Doc number : 45167 34 % Doc number : 45327 34 % Doc number : 45601 34 % Doc number : 46074 % Doc number : 46590 % Doc number : 46782 35 % Doc number : 47194 35 % Doc number : 47433 36 % Doc number : 47531 36 % Doc number : 47957 36 % Doc number : 48235 36 % Doc number : 48535 37 % Doc number : 48822 37 % Doc number : 48966 37 % Doc number : 49213 37 % Doc number : 4949837 % Doc number : 49695 37 % Doc number : 49984 38 % Doc number : 50464Doc number : 50640 38 % Doc number : 50793 38 % Doc number : 50942 51482 39 % Doc number : 51787 39 % Doc number : 52076 39 % Doc number : 52701 40 % Doc number : 52836 40 % Doc number : 52986 Doc number : 53252% Doc number : 53976 % Doc number : 54119 Doc number : 54646 41 % Doc number : 54918 % Doc number : 55348 5605942 % Doc number : 56329 42 % Doc number : 56608 43 % Doc number : 56777 43 % Doc number : 57188 43 % Doc number : 57895 43 % Doc number : 58027 44 % Doc number : 58362 44 % Doc number : 58723 44 % Doc number : 58839 44 % Doc number : 58978Doc number : 59125 59410 45 % Doc number : 5955345 % Doc number : 59817 45 % Doc number : 59953 45 % Doc number : 60479 46 % Doc number : 61013Doc number : 61447 47 % Doc number : 62065 47 % Doc number : 62196% Doc number : 62502 47 % Doc number : 62661 47 % Doc number : 6282047 % Doc number : 62953 47 % Doc number : 6324148 % Doc number : 63541Doc number : 63695% Doc number : 6448865048 49 % Doc number : 65180 Doc number : 65329 65480 49 % Doc number : 65628 % Doc number : 65775 50 % Doc number : 66383Doc number : 66769 Doc number : 66926 50 % Doc number : 67162 51 % Doc number : 67310 51 % Doc number : 67463 51 % Doc number : 68026Doc number : 68310 51 % Doc number : 68451 52 % Doc number : 68587 52 % Doc number : 68872% Doc number : 69297 52 % Doc number : 69426 52 % Doc number : 69569 52 % Doc number : 69705 Doc number : 69852 53 % Doc number : 70004 53 % Doc number : 70144% Doc number : 70232 53 % Doc number : 7069570871 53 % Doc number : 70968 54 % Doc number : 71262 54 % Doc number : 71365 % Doc number : 71586 54 % Doc number : 72076 54 % Doc number : 72274 55 % Doc number : 72615 55 % Doc number : 72748 55 % Doc number : 72980 55 % Doc number : 73088 55 % Doc number : 73176 55 % Doc number : 73285 55 % Doc number : 7351973748 56 % Doc number : 74075 56 % Doc number : 74176 56 % Doc number : 74501 56 % Doc number : 74601% Doc number : 75021 56 % Doc number : 75139 57 % Doc number : 75375 57 % Doc number : 75477 57 % Doc number : 76122 57 % Doc number : 76350 57 % Doc number : 76445 58 % Doc number : 76543 58 % Doc number : 76638 58 % Doc number : 76937 58 % Doc number : 77254 58 % Doc number : 77353 58 % Doc number : 77669 59 % Doc number : 77855 59 % Doc number : 77974 59 % Doc number : 7807578175 59 % Doc number : 7831359 % Doc number : 7847959 % Doc number : 78575 59 % Doc number : 78667 59 % Doc number : 78759 59 % Doc number : 78878 59 % Doc number : 79061 60 % Doc number : 79252 60 % Doc number : 7995960 % Doc number : 80189 60 % Doc number : 80319 61 % Doc number : 80461 61 % Doc number : 80982 61 % Doc number : 81360 62 % Doc number : 81883 62 % Doc number : 82013 Doc number : 8214262 % Doc number : 82241 62 % Doc number : 82643 63 % Doc number : 83679 63 % Doc number : 83809 63 % Doc number : 84146% Doc number : 85839 65 % Doc number : 86197 65 % Doc number : 86369 65 % Doc number : 86839 86980 66 % Doc number : 87646 66 % Doc number : 88021 66 % Doc number : 8819388315 67 % Doc number : 88450 67 % Doc number : 88741 67 % Doc number : 89308 % Doc number : 89759 68 % Doc number : 89917 68 % Doc number : 90136 68 % Doc number : 90489 68 % Doc number : 90589 68 % Doc number : 90719 69 % Doc number : 91144 69 % Doc number : 92031 70 % Doc number : 92502 70 % Doc number : 9262970 % Doc number : 93146 71 % Doc number : 93873 71 % Doc number : 94022 % Doc number : 95219 72 % Doc number : 95568 % Doc number : 95862 72 % Doc number : 96131 73 % Doc number : 96693 73 % Doc number : 97106 73 % Doc number : 97219 73 % Doc number : 9739898676 % Doc number : 98938 75 % Doc number : 9899699171 75 % Doc number : 99741 75 % Doc number : 99881 76 % Doc number : 100509 100683 76 % Doc number : 101277 76 % Doc number : 101480 % Doc number : 103115 78 % Doc number : 103256 78 % Doc number : 103732 79 % Doc number : 104658 79 % Doc number : 105333 80 % Doc number : 105582 105976 80 % Doc number : 106159 80 % Doc number : 106645 80 % Doc number : 106785 Doc number : 107096 82 % Doc number : 108159 108496 82 % Doc number : 109321109689 83 % Doc number : 110409 84 % Doc number : 111109 84 % Doc number : 111452 84 % Doc number : 112008 85 % Doc number : 112205 85 % Doc number : 113030 85 % Doc number : 113186% Doc number : 113525 87 % Doc number : 115891 88 % Doc number : 116277 88 % Doc number : 116835 88 % Doc number : 117010 88 % Doc number : 117185 89 % Doc number : 117947 89 % Doc number : 118418Doc number : 119114 91 % Doc number : 120206 91 % Doc number : 120763 91 % Doc number : 120939 92 % Doc number : 12165792 % Doc number : 121970 92 % Doc number : 122116122825 93 % Doc number : 123173Doc number : 123485 93 % Doc number : 123667% Doc number : 124220 94 % Doc number : 124578 Doc number : 124904 95 % Doc number : 125403 % Doc number : 125565 96 % Doc number : 126702 96 % Doc number : 127003 96 % Doc number : 127180 96 % Doc number : 127569 97 % Doc number : 128146 97 % Doc number : 128336 97 % Doc number : 128501 98 % Doc number : 129303 98 % Doc number : 130411 99 % Doc number : 130972 99 % Doc number : 131149\r",
      "> 99 % Doc number : 131150\r",
      "> 99 % Doc number : 131151\r",
      "> 99 % Doc number : 131152\r",
      "> 99 % Doc number : 131153\r",
      "> 99 % Doc number : 131154\r",
      "> 99 % Doc number : 131155\r",
      "> 99 % Doc number : 131156\r",
      "> 99 % Doc number : 131157\r",
      "> 99 % Doc number : 131158\r",
      "> 99 % Doc number : 131159\r",
      "> 99 % Doc number : 131160\r",
      "> 99 % Doc number : 131161\r",
      "> 99 % Doc number : 131162\r",
      "> 99 % Doc number : 131163\r",
      "> 99 % Doc number : 131164\r",
      "> 99 % Doc number : 131165\r",
      "> 99 % Doc number : 131166\r",
      "> 99 % Doc number : 131167\r",
      "> 99 % Doc number : 131168\r",
      "> 99 % Doc number : 131169\r",
      "> 99 % Doc number : 131170\r",
      "> 99 % Doc number : 131171\r",
      "> 99 % Doc number : 131172\r",
      "> 99 % Doc number : 131173\r",
      "> 99 % Doc number : 131174\r",
      "> 99 % Doc number : 131175\r",
      "> 99 % Doc number : 131176\r",
      "> 99 % Doc number : 131177\r",
      "> 99 % Doc number : 131178\r",
      "> 99 % Doc number : 131179\r",
      "> 99 % Doc number : 131180\r",
      "> 99 % Doc number : 131181\r",
      "> 99 % Doc number : 131182\r",
      "> 99 % Doc number : 131183\r",
      "> 99 % Doc number : 131184\r",
      "> 99 % Doc number : 131185\r",
      "> 99 % Doc number : 131186\r",
      "> 99 % Doc number : 131187\r",
      "> 99 % Doc number : 131188\r",
      "> 99 % Doc number : 131189\r",
      "> 99 % Doc number : 131190\r",
      "> 99 % Doc number : 131191\r",
      "> 99 % Doc number : 131192\r",
      "> 99 % Doc number : 131193\r",
      "> 99 % Doc number : 131194\r",
      "> 99 % Doc number : 131195\r",
      "> 99 % Doc number : 131196\r",
      "> 99 % Doc number : 131197\r",
      "> 99 % Doc number : 131198\r",
      "> 99 % Doc number : 131199\r",
      "> 99 % Doc number : 131200\r",
      "> 99 % Doc number : 131201\r",
      "> 99 % Doc number : 131202\r",
      "> 99 % Doc number : 131203\r",
      "> 99 % Doc number : 131204\r",
      "> 99 % Doc number : 131205\r",
      "> 99 % Doc number : 131206\r",
      "> 99 % Doc number : 131207\r",
      "> 99 % Doc number : 131208\r",
      "> 99 % Doc number : 131209\r",
      "> 99 % Doc number : 131210\r",
      "> 99 % Doc number : 131211\r",
      "> 99 % Doc number : 131212\r",
      "> 99 % Doc number : 131213\r",
      "> 99 % Doc number : 131214\r",
      "> 99 % Doc number : 131215\r",
      "> 99 % Doc number : 131216\r",
      "> 99 % Doc number : 131217\r",
      "> 99 % Doc number : 131218\r",
      "> 99 % Doc number : 131219\r",
      "> 99 % Doc number : 131220\r",
      "> 99 % Doc number : 131221\r",
      "> 99 % Doc number : 131222\r",
      "> 99 % Doc number : 131223\r",
      "> 99 % Doc number : 131224\r",
      "> 99 % Doc number : 131225\r",
      "> 99 % Doc number : 131226\r",
      "> 99 % Doc number : 131227\r",
      "> 99 % Doc number : 131228\r",
      "> 99 % Doc number : 131229\r",
      "> 99 % Doc number : 131230\r",
      "> 99 % Doc number : 131231\r",
      "> 99 % Doc number : 131232\r",
      "> 99 % Doc number : 131233\r",
      "> 99 % Doc number : 131234\r",
      "> 99 % Doc number : 131235\r",
      "> 99 % Doc number : 131236\r",
      "> 99 % Doc number : 131237\r",
      "> 99 % Doc number : 131238\r",
      "> 99 % Doc number : 131239\r",
      "> 99 % Doc number : 131240\r",
      "> 99 % Doc number : 131241\r",
      "> 99 % Doc number : 131242\r",
      "> 99 % Doc number : 131243\r",
      "> 99 % Doc number : 131244\r",
      "> 99 % Doc number : 131245\r",
      "> 99 % Doc number : 131246\r",
      "> 99 % Doc number : 131247\r",
      "> 99 % Doc number : 131248\r",
      "> 99 % Doc number : 131249\r",
      "> 99 % Doc number : 131250\r",
      "> 99 % Doc number : 131251\r",
      "> 99 % Doc number : 131252\r",
      "> 99 % Doc number : 131253\r",
      "> 99 % Doc number : 131254\r",
      "> 99 % Doc number : 131255\r",
      "> 99 % Doc number : 131256\r",
      "> 99 % Doc number : 131257\r",
      "> 99 % Doc number : 131258\r",
      "> 99 % Doc number : 131259\r",
      "> 99 % Doc number : 131260\r",
      "> 99 % Doc number : 131261\r",
      "> 99 % Doc number : 131262\r",
      "> 99 % Doc number : 131263\r",
      "> 99 % Doc number : 131264\r",
      "> 99 % Doc number : 131265\r",
      "> 99 % Doc number : 131266\r",
      "> 99 % Doc number : 131267\r",
      "> 99 % Doc number : 131268\r",
      "> 99 % Doc number : 131269\r",
      "> 99 % Doc number : 131270\r",
      "> 99 % Doc number : 131271\r",
      "> 99 % Doc number : 131272\r",
      "> 99 % Doc number : 131273\r",
      "> 99 % Doc number : 131274\r",
      "> 99 % Doc number : 131275\r",
      "> 99 % Doc number : 131276\r",
      "> 99 % Doc number : 131277\r",
      "> 99 % Doc number : 131278\r",
      "> 99 % Doc number : 131279\r",
      "> 99 % Doc number : 131280\r",
      "> 99 % Doc number : 131281\r",
      "> 99 % Doc number : 131282\r",
      "> 99 % Doc number : 131283\r",
      "> 99 % Doc number : 131284\r",
      "> 99 % Doc number : 131285\r",
      "> 99 % Doc number : 131286\r",
      "> 99 % Doc number : 131287\r",
      "> 99 % Doc number : 131288\r",
      "> 99 % Doc number : 131289\r",
      "> 99 % Doc number : 131290\r",
      "> 99 % Doc number : 131291\r",
      "> 99 % Doc number : 131292\r",
      "> 99 % Doc number : 131293\r",
      "> 99 % Doc number : 131294\r",
      "> 99 % Doc number : 131295\r",
      "> 99 % Doc number : 131296\r",
      "> 99 % Doc number : 131297\r",
      "> 99 % Doc number : 131298\r",
      "> 99 % Doc number : 131299\r",
      "> 99 % Doc number : 131300\r",
      "> 99 % Doc number : 131301\r",
      "> 99 % Doc number : 131302\r",
      "> 99 % Doc number : 131303\r",
      "> 99 % Doc number : 131304\r",
      "> 99 % Doc number : 131305\r",
      "> 99 % Doc number : 131306\r",
      "> 99 % Doc number : 131307\r",
      "> 99 % Doc number : 131308\r",
      "> 99 % Doc number : 131309\r",
      "> 99 % Doc number : 131310\r",
      "> 99 % Doc number : 131311\r",
      "> 99 % Doc number : 131312\r",
      "> 99 % Doc number : 131313\r",
      "> 99 % Doc number : 131314\r",
      "> 99 % Doc number : 131315\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 % Doc number : 14189796 100 % Doc number : 132015 100 % Doc number : 132385 100 % Doc number : 132868133041 101 % Doc number : 133406 101 % Doc number : 134374 102 % Doc number : 135034 102 % Doc number : 135805 103 % Doc number : 136926 % Doc number : 137305 137494 137667138140 105 % Doc number : 139165 105 % Doc number : 139366% Doc number : 139529 106 % Doc number : 140800 106 % Doc number : 141127 107 % Doc number : 141820\n",
      "> start merging runs' results\n",
      "> 14 temporary files opened\n",
      "> Word processed : 202026ord processed : 5943 Word processed : 7907Word processed : 91605 95060 Word processed : 95442Word processed : 104769Word processed : 112644 113511113629Word processed : 115287 115914 124272 125288 Word processed : 142470\r"
     ]
    }
   ],
   "source": [
    "tokenizer_ = tokenizerCpp.Tokenizer(datasetFoldername, lemmatization_ = False, stemming_ = False)\n",
    "#set runSize such that :\n",
    "#the total number of documents (~130 000) in the dataset divided by runSize is less than the allowed number of \n",
    "#simultaneously opened files on your machine (usually 1024) \n",
    "runSize_ = 10000\n",
    "\n",
    "start_time = time.time()\n",
    "#ifConstructor.constructIF_diskBased(tokenizer_, runSize = runSize_, score_tf_idf = True)\n",
    "cProfile.run(\"ifConstructor.constructIF_diskBased(tokenizer_, runSize = runSize_, score_tf_idf = True)\")\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runSize_ = 130\n",
    "\n",
    "cProfile.run(\"ifConstructor.constructIF_diskBased(tokenizer_, runSize = runSize_, score_tf_idf = True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION\n",
    "\n",
    "### Time to run algorithm\n",
    "\n",
    "In this section, we will use neither stemming/lemmatization nor word embedding. We will also use the query \"Chocolate and internet\" for all algorithm.\n",
    "\n",
    "We compute the naive algorithm on this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the fagin algorithm on the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONCLUSION\n",
    "\n",
    "### Time to run algorithm\n",
    "\n",
    "In this section, we will use neither stemming/lemmatization nor word embedding. We will also use the query \"Chocolate and internet\" for all algorithm.\n",
    "\n",
    "We compute the naive algorithm on this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the fagin algorithm on the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testTime(queries):\n",
    "    start_time = time.time()\n",
    "    for query in queries:\n",
    "        algo.naiveAlgo(query)\n",
    "    print(\"--- %s naiveAlgo seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    for query in queries:\n",
    "        algo.faginAlgo(query)\n",
    "    print(\"--- %s faginAlgo seconds ---\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    for query in queries:\n",
    "        algo.threshold(query)\n",
    "    print(\"--- %s threshold seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.loadVocabulary(\"./Globals/nostemm_nolemm_tf_idf/vocabulary.dict\",\"./Globals/nostemm_nolemm_tf_idf/IF.dict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneWord = [\n",
    "        [(\"daylight\",3)]\n",
    "    ]\n",
    "\n",
    "notExist = [[(\"fdadfdfewf\",3)],\n",
    "           [(\"114rf4434\",3)],\n",
    "            [(\"jdifjoiq2323\",3)]\n",
    "           ]\n",
    "\n",
    "queries = [\n",
    "                [(\"love\",3), (\"chocolate\",3)],\n",
    "                [(\"january\",3)],\n",
    "                [(\"narrow\",3)],\n",
    "                [(\"today\",3), (\"tomorrow\",3)]           \n",
    "    ]\n",
    "\n",
    "queries1 = [\n",
    "      [(\"love\",3), (\"and\",3), (\"chocolate\",3)],\n",
    "                [(\"january\",3)],\n",
    "                [(\"narrow\",3)],\n",
    "                [(\"today\",3), (\"and\",3), (\"tomorrow\",3)],\n",
    "\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the three algos on the words not existing in the dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 4.124641418457031e-05 naiveAlgo seconds ---\n",
      "--- 0.0005621910095214844 faginAlgo seconds ---\n",
      "--- 0.00039196014404296875 threshold seconds ---\n"
     ]
    }
   ],
   "source": [
    "testTime(notExist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result obtained:\n",
    "\n",
    "--- 4.124641418457031e-05 naiveAlgo seconds ---  \n",
    "--- 0.0005621910095214844 faginAlgo seconds ---  \n",
    "--- 0.00039196014404296875 threshold seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the three algos with one word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testTime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0b2cffbf18bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtestTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moneWord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'testTime' is not defined"
     ]
    }
   ],
   "source": [
    "testTime(oneWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- 0.002106189727783203 naiveAlgo seconds ---\n",
    "--- 0.004480123519897461 faginAlgo seconds ---\n",
    "--- 0.0021691322326660156 threshold seconds ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the three algos with random words\n",
    "Remark: We notice that the fagin algo is quite slow because it needs to go through every posting list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.45010828971862793 naiveAlgo seconds ---\n",
      "--- 8.133760929107666 faginAlgo seconds ---\n",
      "--- 0.44022607803344727 threshold seconds ---\n"
     ]
    }
   ],
   "source": [
    "testTime(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- 0.45010828971862793 naiveAlgo seconds ---  \n",
    "--- 8.133760929107666 faginAlgo seconds ---  \n",
    "--- 0.44022607803344727 threshold seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
